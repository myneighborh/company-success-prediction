{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID 컬럼 분리\n",
    "train = train.drop(columns=['ID'], axis = 1)\n",
    "test = test.drop(columns=['ID'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>설립연도</th>\n",
       "      <th>국가</th>\n",
       "      <th>분야</th>\n",
       "      <th>투자단계</th>\n",
       "      <th>직원 수</th>\n",
       "      <th>인수여부</th>\n",
       "      <th>상장여부</th>\n",
       "      <th>고객수(백만명)</th>\n",
       "      <th>총 투자금(억원)</th>\n",
       "      <th>연매출(억원)</th>\n",
       "      <th>SNS 팔로워 수(백만명)</th>\n",
       "      <th>기업가치(백억원)</th>\n",
       "      <th>성공확률</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009</td>\n",
       "      <td>CT005</td>\n",
       "      <td>이커머스</td>\n",
       "      <td>Series A</td>\n",
       "      <td>4126.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>56.0</td>\n",
       "      <td>3365.0</td>\n",
       "      <td>4764.0</td>\n",
       "      <td>4.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>CT006</td>\n",
       "      <td>핀테크</td>\n",
       "      <td>Seed</td>\n",
       "      <td>4167.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>80.0</td>\n",
       "      <td>4069.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2500-3500</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>CT007</td>\n",
       "      <td>기술</td>\n",
       "      <td>Series A</td>\n",
       "      <td>3132.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6453.0</td>\n",
       "      <td>12141.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3500-4500</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>CT006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seed</td>\n",
       "      <td>3245.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>665.0</td>\n",
       "      <td>10547.0</td>\n",
       "      <td>2.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>CT002</td>\n",
       "      <td>에듀테크</td>\n",
       "      <td>Seed</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>94.0</td>\n",
       "      <td>829.0</td>\n",
       "      <td>9810.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1500-2500</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   설립연도     국가    분야      투자단계    직원 수 인수여부 상장여부  고객수(백만명)  총 투자금(억원)  \\\n",
       "0  2009  CT005  이커머스  Series A  4126.0   No   No      56.0     3365.0   \n",
       "1  2023  CT006   핀테크      Seed  4167.0  Yes   No      80.0     4069.0   \n",
       "2  2018  CT007    기술  Series A  3132.0  Yes  Yes      54.0     6453.0   \n",
       "3  2016  CT006   NaN      Seed  3245.0  Yes  Yes       NaN      665.0   \n",
       "4  2020  CT002  에듀테크      Seed  1969.0   No  Yes      94.0      829.0   \n",
       "\n",
       "   연매출(억원)  SNS 팔로워 수(백만명)  기업가치(백억원)  성공확률  \n",
       "0   4764.0            4.71        NaN   0.3  \n",
       "1    279.0            1.00  2500-3500   0.8  \n",
       "2  12141.0            4.00  3500-4500   0.5  \n",
       "3  10547.0            2.97        NaN   0.7  \n",
       "4   9810.0            1.00  1500-2500   0.1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_valuation(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    if '이상' in val:\n",
    "        # '6000이상' → 6000\n",
    "        return int(re.sub('[^0-9]', '', val))\n",
    "    elif '-' in val:\n",
    "        # '2500-3500' → 평균값 계산\n",
    "        low, high = map(int, val.split('-'))\n",
    "        return (low + high) / 2\n",
    "    else:\n",
    "        # 숫자로 변환 가능한 경우\n",
    "        try:\n",
    "            return float(val)\n",
    "        except:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 설립연도 -> 연차로 변경\n",
    "current_year = 2025\n",
    "\n",
    "train['연차'] = current_year - train['설립연도']\n",
    "test['연차'] = current_year - test['설립연도']\n",
    "\n",
    "# 설립연도 제거\n",
    "train.drop(columns = ['설립연도'], inplace = True)\n",
    "test.drop(columns = ['설립연도'], inplace = True)\n",
    "\n",
    "category_features = ['국가','분야']\n",
    "numeric_features = ['연차', '투자단계', '직원 수','고객수(백만명)','총 투자금(억원)','연매출(억원)','SNS 팔로워 수(백만명)', '기업가치(백억원)']\n",
    "bool_features = ['인수여부','상장여부']\n",
    "\n",
    "# 투자단계 순서를 숫자로 매핑\n",
    "investment_stage_map = {\n",
    "    'Seed': 0,\n",
    "    'Series A': 1,\n",
    "    'Series B': 2,\n",
    "    'Series C': 3,\n",
    "    'IPO': 4,\n",
    "    'Missing': -1\n",
    "}\n",
    "\n",
    "# 결측치 먼저 처리 후 매핑\n",
    "train['투자단계'] = train['투자단계'].fillna('Missing').map(investment_stage_map)\n",
    "test['투자단계'] = test['투자단계'].fillna('Missing').map(investment_stage_map)\n",
    "\n",
    "# 기업가치 변환\n",
    "train['기업가치(백억원)'] = train['기업가치(백억원)'].apply(clean_valuation)\n",
    "test['기업가치(백억원)'] = test['기업가치(백억원)'].apply(clean_valuation)\n",
    "\n",
    "# LabelEncoder 객체를 각 범주형 feature별로 따로 저장하여 사용\n",
    "encoders = {}\n",
    "\n",
    "# 범주형 데이터를 encoding\n",
    "for feature in category_features:\n",
    "    encoders[feature] = LabelEncoder()\n",
    "    train[feature] = train[feature].fillna('Missing')\n",
    "    test[feature] = test[feature].fillna('Missing')\n",
    "    train[feature] = encoders[feature].fit_transform(train[feature])\n",
    "    test[feature] = encoders[feature].transform(test[feature])\n",
    "\n",
    "# 불리언 값을 0과 1로 변환 ('Yes' → 1, 'No' → 0 으로 변환)\n",
    "bool_map = {'Yes': 1, 'No': 0}\n",
    "\n",
    "for feature in bool_features:\n",
    "    train[feature] = train[feature].map(bool_map)\n",
    "    test[feature] = test[feature].map(bool_map)\n",
    "\n",
    "# 수치형 변수 결측치를 중간값으로 대체\n",
    "for feature in numeric_features:\n",
    "    median_value = train[feature].median()\n",
    "    train[feature] = train[feature].fillna(median_value)\n",
    "    test[feature] = test[feature].fillna(median_value)\n",
    "\n",
    "# TabNet용 범주형 변수 인덱스(cat_idxs) 및 차원(cat_dims) 설정\n",
    "features = [col for col in train.columns if col != '성공확률']\n",
    "cat_idxs = [features.index(col) for col in category_features]\n",
    "cat_dims = [train[col].max() + 1 for col in category_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>국가</th>\n",
       "      <th>분야</th>\n",
       "      <th>투자단계</th>\n",
       "      <th>직원 수</th>\n",
       "      <th>인수여부</th>\n",
       "      <th>상장여부</th>\n",
       "      <th>고객수(백만명)</th>\n",
       "      <th>총 투자금(억원)</th>\n",
       "      <th>연매출(억원)</th>\n",
       "      <th>SNS 팔로워 수(백만명)</th>\n",
       "      <th>기업가치(백억원)</th>\n",
       "      <th>성공확률</th>\n",
       "      <th>연차</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4126.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>3365.0</td>\n",
       "      <td>4764.0</td>\n",
       "      <td>4.71</td>\n",
       "      <td>4049.984157</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4167.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4069.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3132.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>6453.0</td>\n",
       "      <td>12141.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3245.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>49.214332</td>\n",
       "      <td>665.0</td>\n",
       "      <td>10547.0</td>\n",
       "      <td>2.97</td>\n",
       "      <td>4049.984157</td>\n",
       "      <td>0.7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>829.0</td>\n",
       "      <td>9810.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   국가  분야  투자단계    직원 수  인수여부  상장여부   고객수(백만명)  총 투자금(억원)  연매출(억원)  \\\n",
       "0   4   7     1  4126.0     0     0  56.000000     3365.0   4764.0   \n",
       "1   5   9     0  4167.0     1     0  80.000000     4069.0    279.0   \n",
       "2   6   3     1  3132.0     1     1  54.000000     6453.0  12141.0   \n",
       "3   5   1     0  3245.0     1     1  49.214332      665.0  10547.0   \n",
       "4   1   6     0  1969.0     0     1  94.000000      829.0   9810.0   \n",
       "\n",
       "   SNS 팔로워 수(백만명)    기업가치(백억원)  성공확률  연차  \n",
       "0            4.71  4049.984157   0.3  16  \n",
       "1            1.00  3000.000000   0.8   2  \n",
       "2            4.00  4000.000000   0.5   7  \n",
       "3            2.97  4049.984157   0.7   9  \n",
       "4            1.00  2000.000000   0.1   5  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "🔁 Fold 1/10\n",
      "▶ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Fine-tuning...\n",
      "epoch 0  | loss: 9.75046 | val_0_mae: 1.88839 |  0:00:00s\n",
      "epoch 1  | loss: 1.61759 | val_0_mae: 2.18384 |  0:00:00s\n",
      "epoch 2  | loss: 0.70409 | val_0_mae: 1.40964 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.31501 | val_0_mae: 0.94061 |  0:00:00s\n",
      "epoch 4  | loss: 0.19432 | val_0_mae: 1.16273 |  0:00:00s\n",
      "epoch 5  | loss: 0.12828 | val_0_mae: 1.22528 |  0:00:00s\n",
      "epoch 6  | loss: 0.10042 | val_0_mae: 0.93654 |  0:00:00s\n",
      "epoch 7  | loss: 0.09095 | val_0_mae: 0.84326 |  0:00:00s\n",
      "epoch 8  | loss: 0.07859 | val_0_mae: 0.74689 |  0:00:00s\n",
      "epoch 9  | loss: 0.0702  | val_0_mae: 0.64778 |  0:00:00s\n",
      "epoch 10 | loss: 0.06966 | val_0_mae: 0.61959 |  0:00:00s\n",
      "epoch 11 | loss: 0.06756 | val_0_mae: 0.53817 |  0:00:00s\n",
      "epoch 12 | loss: 0.06546 | val_0_mae: 0.44731 |  0:00:00s\n",
      "epoch 13 | loss: 0.06788 | val_0_mae: 0.44165 |  0:00:00s\n",
      "epoch 14 | loss: 0.06518 | val_0_mae: 0.41787 |  0:00:00s\n",
      "epoch 15 | loss: 0.06392 | val_0_mae: 0.38005 |  0:00:00s\n",
      "epoch 16 | loss: 0.06155 | val_0_mae: 0.36391 |  0:00:00s\n",
      "epoch 17 | loss: 0.06265 | val_0_mae: 0.34463 |  0:00:00s\n",
      "epoch 18 | loss: 0.06145 | val_0_mae: 0.33018 |  0:00:00s\n",
      "epoch 19 | loss: 0.06156 | val_0_mae: 0.30239 |  0:00:00s\n",
      "epoch 20 | loss: 0.06124 | val_0_mae: 0.28173 |  0:00:00s\n",
      "epoch 21 | loss: 0.06219 | val_0_mae: 0.28897 |  0:00:01s\n",
      "epoch 22 | loss: 0.05984 | val_0_mae: 0.28182 |  0:00:01s\n",
      "epoch 23 | loss: 0.06028 | val_0_mae: 0.26983 |  0:00:01s\n",
      "epoch 24 | loss: 0.06058 | val_0_mae: 0.2558  |  0:00:01s\n",
      "epoch 25 | loss: 0.06096 | val_0_mae: 0.25805 |  0:00:01s\n",
      "epoch 26 | loss: 0.06124 | val_0_mae: 0.26055 |  0:00:01s\n",
      "epoch 27 | loss: 0.06081 | val_0_mae: 0.24875 |  0:00:01s\n",
      "epoch 28 | loss: 0.06147 | val_0_mae: 0.24526 |  0:00:01s\n",
      "epoch 29 | loss: 0.06128 | val_0_mae: 0.24257 |  0:00:01s\n",
      "epoch 30 | loss: 0.06042 | val_0_mae: 0.23908 |  0:00:01s\n",
      "epoch 31 | loss: 0.06126 | val_0_mae: 0.23203 |  0:00:01s\n",
      "epoch 32 | loss: 0.06023 | val_0_mae: 0.2403  |  0:00:01s\n",
      "epoch 33 | loss: 0.05971 | val_0_mae: 0.22449 |  0:00:01s\n",
      "epoch 34 | loss: 0.06057 | val_0_mae: 0.23017 |  0:00:01s\n",
      "epoch 35 | loss: 0.05947 | val_0_mae: 0.21926 |  0:00:01s\n",
      "epoch 36 | loss: 0.05996 | val_0_mae: 0.21989 |  0:00:01s\n",
      "epoch 37 | loss: 0.06022 | val_0_mae: 0.21875 |  0:00:01s\n",
      "epoch 38 | loss: 0.06061 | val_0_mae: 0.2207  |  0:00:01s\n",
      "epoch 39 | loss: 0.05946 | val_0_mae: 0.21662 |  0:00:01s\n",
      "epoch 40 | loss: 0.05953 | val_0_mae: 0.21254 |  0:00:01s\n",
      "epoch 41 | loss: 0.06018 | val_0_mae: 0.21003 |  0:00:01s\n",
      "epoch 42 | loss: 0.05945 | val_0_mae: 0.21448 |  0:00:01s\n",
      "epoch 43 | loss: 0.05968 | val_0_mae: 0.20498 |  0:00:01s\n",
      "epoch 44 | loss: 0.05876 | val_0_mae: 0.20587 |  0:00:02s\n",
      "epoch 45 | loss: 0.0593  | val_0_mae: 0.20559 |  0:00:02s\n",
      "epoch 46 | loss: 0.05975 | val_0_mae: 0.20363 |  0:00:02s\n",
      "epoch 47 | loss: 0.05928 | val_0_mae: 0.20663 |  0:00:02s\n",
      "epoch 48 | loss: 0.05962 | val_0_mae: 0.20384 |  0:00:02s\n",
      "epoch 49 | loss: 0.05941 | val_0_mae: 0.20474 |  0:00:02s\n",
      "epoch 50 | loss: 0.05931 | val_0_mae: 0.19934 |  0:00:02s\n",
      "epoch 51 | loss: 0.06035 | val_0_mae: 0.20511 |  0:00:02s\n",
      "epoch 52 | loss: 0.05925 | val_0_mae: 0.1997  |  0:00:02s\n",
      "epoch 53 | loss: 0.05919 | val_0_mae: 0.20374 |  0:00:02s\n",
      "epoch 54 | loss: 0.05917 | val_0_mae: 0.20284 |  0:00:02s\n",
      "epoch 55 | loss: 0.05919 | val_0_mae: 0.20381 |  0:00:02s\n",
      "epoch 56 | loss: 0.05892 | val_0_mae: 0.19957 |  0:00:02s\n",
      "epoch 57 | loss: 0.05888 | val_0_mae: 0.206   |  0:00:02s\n",
      "epoch 58 | loss: 0.05957 | val_0_mae: 0.201   |  0:00:02s\n",
      "epoch 59 | loss: 0.06094 | val_0_mae: 0.20276 |  0:00:02s\n",
      "epoch 60 | loss: 0.05951 | val_0_mae: 0.19908 |  0:00:02s\n",
      "epoch 61 | loss: 0.05846 | val_0_mae: 0.20033 |  0:00:02s\n",
      "epoch 62 | loss: 0.05913 | val_0_mae: 0.19987 |  0:00:02s\n",
      "epoch 63 | loss: 0.05873 | val_0_mae: 0.19936 |  0:00:02s\n",
      "epoch 64 | loss: 0.05794 | val_0_mae: 0.19969 |  0:00:02s\n",
      "epoch 65 | loss: 0.05939 | val_0_mae: 0.20089 |  0:00:02s\n",
      "epoch 66 | loss: 0.05865 | val_0_mae: 0.20074 |  0:00:02s\n",
      "epoch 67 | loss: 0.05831 | val_0_mae: 0.20096 |  0:00:02s\n",
      "epoch 68 | loss: 0.0593  | val_0_mae: 0.20082 |  0:00:03s\n",
      "epoch 69 | loss: 0.05896 | val_0_mae: 0.20031 |  0:00:03s\n",
      "epoch 70 | loss: 0.05872 | val_0_mae: 0.19955 |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 70 with best_epoch = 60 and best_val_0_mae = 0.19908\n",
      "\n",
      "🔁 Fold 2/10\n",
      "▶ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Fine-tuning...\n",
      "epoch 0  | loss: 4.18775 | val_0_mae: 0.98624 |  0:00:00s\n",
      "epoch 1  | loss: 0.82364 | val_0_mae: 0.67322 |  0:00:00s\n",
      "epoch 2  | loss: 0.38889 | val_0_mae: 0.76104 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.16747 | val_0_mae: 0.39123 |  0:00:00s\n",
      "epoch 4  | loss: 0.12381 | val_0_mae: 0.43226 |  0:00:00s\n",
      "epoch 5  | loss: 0.10231 | val_0_mae: 0.30735 |  0:00:00s\n",
      "epoch 6  | loss: 0.08949 | val_0_mae: 0.30989 |  0:00:00s\n",
      "epoch 7  | loss: 0.07885 | val_0_mae: 0.27362 |  0:00:00s\n",
      "epoch 8  | loss: 0.07322 | val_0_mae: 0.24294 |  0:00:00s\n",
      "epoch 9  | loss: 0.06925 | val_0_mae: 0.23566 |  0:00:00s\n",
      "epoch 10 | loss: 0.06934 | val_0_mae: 0.23672 |  0:00:00s\n",
      "epoch 11 | loss: 0.06684 | val_0_mae: 0.23535 |  0:00:00s\n",
      "epoch 12 | loss: 0.06552 | val_0_mae: 0.23731 |  0:00:00s\n",
      "epoch 13 | loss: 0.06328 | val_0_mae: 0.21843 |  0:00:00s\n",
      "epoch 14 | loss: 0.0639  | val_0_mae: 0.21734 |  0:00:00s\n",
      "epoch 15 | loss: 0.06229 | val_0_mae: 0.22649 |  0:00:00s\n",
      "epoch 16 | loss: 0.06276 | val_0_mae: 0.21554 |  0:00:00s\n",
      "epoch 17 | loss: 0.06201 | val_0_mae: 0.21416 |  0:00:00s\n",
      "epoch 18 | loss: 0.06348 | val_0_mae: 0.21856 |  0:00:00s\n",
      "epoch 19 | loss: 0.05984 | val_0_mae: 0.22202 |  0:00:00s\n",
      "epoch 20 | loss: 0.06182 | val_0_mae: 0.21898 |  0:00:00s\n",
      "epoch 21 | loss: 0.0609  | val_0_mae: 0.21442 |  0:00:00s\n",
      "epoch 22 | loss: 0.06164 | val_0_mae: 0.21596 |  0:00:01s\n",
      "epoch 23 | loss: 0.06107 | val_0_mae: 0.21559 |  0:00:01s\n",
      "epoch 24 | loss: 0.05903 | val_0_mae: 0.2131  |  0:00:01s\n",
      "epoch 25 | loss: 0.05976 | val_0_mae: 0.21195 |  0:00:01s\n",
      "epoch 26 | loss: 0.0588  | val_0_mae: 0.21283 |  0:00:01s\n",
      "epoch 27 | loss: 0.0597  | val_0_mae: 0.21291 |  0:00:01s\n",
      "epoch 28 | loss: 0.05933 | val_0_mae: 0.21376 |  0:00:01s\n",
      "epoch 29 | loss: 0.05973 | val_0_mae: 0.21617 |  0:00:01s\n",
      "epoch 30 | loss: 0.06032 | val_0_mae: 0.21421 |  0:00:01s\n",
      "epoch 31 | loss: 0.06083 | val_0_mae: 0.21365 |  0:00:01s\n",
      "epoch 32 | loss: 0.05877 | val_0_mae: 0.21345 |  0:00:01s\n",
      "epoch 33 | loss: 0.05849 | val_0_mae: 0.21167 |  0:00:01s\n",
      "epoch 34 | loss: 0.05863 | val_0_mae: 0.20912 |  0:00:01s\n",
      "epoch 35 | loss: 0.05946 | val_0_mae: 0.21451 |  0:00:01s\n",
      "epoch 36 | loss: 0.05932 | val_0_mae: 0.20851 |  0:00:01s\n",
      "epoch 37 | loss: 0.05892 | val_0_mae: 0.21261 |  0:00:01s\n",
      "epoch 38 | loss: 0.0586  | val_0_mae: 0.21532 |  0:00:01s\n",
      "epoch 39 | loss: 0.05762 | val_0_mae: 0.21228 |  0:00:01s\n",
      "epoch 40 | loss: 0.05833 | val_0_mae: 0.213   |  0:00:01s\n",
      "epoch 41 | loss: 0.05891 | val_0_mae: 0.21877 |  0:00:01s\n",
      "epoch 42 | loss: 0.05976 | val_0_mae: 0.21676 |  0:00:01s\n",
      "epoch 43 | loss: 0.05964 | val_0_mae: 0.21583 |  0:00:01s\n",
      "epoch 44 | loss: 0.05888 | val_0_mae: 0.21454 |  0:00:01s\n",
      "epoch 45 | loss: 0.05923 | val_0_mae: 0.21638 |  0:00:01s\n",
      "epoch 46 | loss: 0.05932 | val_0_mae: 0.21494 |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_mae = 0.20851\n",
      "\n",
      "🔁 Fold 3/10\n",
      "▶ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Fine-tuning...\n",
      "epoch 0  | loss: 11.10798| val_0_mae: 1.25535 |  0:00:00s\n",
      "epoch 1  | loss: 1.65661 | val_0_mae: 0.58924 |  0:00:00s\n",
      "epoch 2  | loss: 0.43875 | val_0_mae: 0.62683 |  0:00:00s\n",
      "epoch 3  | loss: 0.22724 | val_0_mae: 0.79719 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4  | loss: 0.14166 | val_0_mae: 0.59279 |  0:00:00s\n",
      "epoch 5  | loss: 0.10119 | val_0_mae: 0.48227 |  0:00:00s\n",
      "epoch 6  | loss: 0.09552 | val_0_mae: 0.34663 |  0:00:00s\n",
      "epoch 7  | loss: 0.07253 | val_0_mae: 0.28092 |  0:00:00s\n",
      "epoch 8  | loss: 0.07659 | val_0_mae: 0.27806 |  0:00:00s\n",
      "epoch 9  | loss: 0.06976 | val_0_mae: 0.23746 |  0:00:00s\n",
      "epoch 10 | loss: 0.06929 | val_0_mae: 0.2537  |  0:00:00s\n",
      "epoch 11 | loss: 0.06581 | val_0_mae: 0.22823 |  0:00:00s\n",
      "epoch 12 | loss: 0.06478 | val_0_mae: 0.23972 |  0:00:00s\n",
      "epoch 13 | loss: 0.06292 | val_0_mae: 0.22152 |  0:00:00s\n",
      "epoch 14 | loss: 0.06051 | val_0_mae: 0.23276 |  0:00:00s\n",
      "epoch 15 | loss: 0.06177 | val_0_mae: 0.21741 |  0:00:00s\n",
      "epoch 16 | loss: 0.06063 | val_0_mae: 0.22927 |  0:00:00s\n",
      "epoch 17 | loss: 0.06196 | val_0_mae: 0.21485 |  0:00:00s\n",
      "epoch 18 | loss: 0.06055 | val_0_mae: 0.22251 |  0:00:00s\n",
      "epoch 19 | loss: 0.0611  | val_0_mae: 0.21204 |  0:00:00s\n",
      "epoch 20 | loss: 0.05966 | val_0_mae: 0.21603 |  0:00:00s\n",
      "epoch 21 | loss: 0.05849 | val_0_mae: 0.21848 |  0:00:01s\n",
      "epoch 22 | loss: 0.0603  | val_0_mae: 0.21489 |  0:00:01s\n",
      "epoch 23 | loss: 0.0591  | val_0_mae: 0.21404 |  0:00:01s\n",
      "epoch 24 | loss: 0.05922 | val_0_mae: 0.21669 |  0:00:01s\n",
      "epoch 25 | loss: 0.0599  | val_0_mae: 0.20967 |  0:00:01s\n",
      "epoch 26 | loss: 0.05828 | val_0_mae: 0.2127  |  0:00:01s\n",
      "epoch 27 | loss: 0.05849 | val_0_mae: 0.21062 |  0:00:01s\n",
      "epoch 28 | loss: 0.05864 | val_0_mae: 0.21313 |  0:00:01s\n",
      "epoch 29 | loss: 0.05818 | val_0_mae: 0.20966 |  0:00:01s\n",
      "epoch 30 | loss: 0.05802 | val_0_mae: 0.21592 |  0:00:01s\n",
      "epoch 31 | loss: 0.05869 | val_0_mae: 0.21106 |  0:00:01s\n",
      "epoch 32 | loss: 0.0607  | val_0_mae: 0.21293 |  0:00:01s\n",
      "epoch 33 | loss: 0.05873 | val_0_mae: 0.21283 |  0:00:01s\n",
      "epoch 34 | loss: 0.05896 | val_0_mae: 0.2125  |  0:00:01s\n",
      "epoch 35 | loss: 0.0586  | val_0_mae: 0.22091 |  0:00:01s\n",
      "epoch 36 | loss: 0.06012 | val_0_mae: 0.21317 |  0:00:01s\n",
      "epoch 37 | loss: 0.05912 | val_0_mae: 0.21339 |  0:00:01s\n",
      "epoch 38 | loss: 0.05836 | val_0_mae: 0.21321 |  0:00:01s\n",
      "epoch 39 | loss: 0.05811 | val_0_mae: 0.21447 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_0_mae = 0.20966\n",
      "\n",
      "🔁 Fold 4/10\n",
      "▶ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Fine-tuning...\n",
      "epoch 0  | loss: 4.54647 | val_0_mae: 0.6813  |  0:00:00s\n",
      "epoch 1  | loss: 0.63129 | val_0_mae: 0.48261 |  0:00:00s\n",
      "epoch 2  | loss: 0.23344 | val_0_mae: 0.90957 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.14892 | val_0_mae: 0.61017 |  0:00:00s\n",
      "epoch 4  | loss: 0.10334 | val_0_mae: 0.445   |  0:00:00s\n",
      "epoch 5  | loss: 0.09102 | val_0_mae: 0.36818 |  0:00:00s\n",
      "epoch 6  | loss: 0.08382 | val_0_mae: 0.46324 |  0:00:00s\n",
      "epoch 7  | loss: 0.07508 | val_0_mae: 0.38998 |  0:00:00s\n",
      "epoch 8  | loss: 0.07185 | val_0_mae: 0.28595 |  0:00:00s\n",
      "epoch 9  | loss: 0.06895 | val_0_mae: 0.25508 |  0:00:00s\n",
      "epoch 10 | loss: 0.06794 | val_0_mae: 0.22236 |  0:00:00s\n",
      "epoch 11 | loss: 0.0691  | val_0_mae: 0.22382 |  0:00:00s\n",
      "epoch 12 | loss: 0.06603 | val_0_mae: 0.24949 |  0:00:00s\n",
      "epoch 13 | loss: 0.06621 | val_0_mae: 0.26609 |  0:00:00s\n",
      "epoch 14 | loss: 0.06416 | val_0_mae: 0.2389  |  0:00:00s\n",
      "epoch 15 | loss: 0.0649  | val_0_mae: 0.23658 |  0:00:00s\n",
      "epoch 16 | loss: 0.0643  | val_0_mae: 0.22636 |  0:00:00s\n",
      "epoch 17 | loss: 0.06344 | val_0_mae: 0.22872 |  0:00:00s\n",
      "epoch 18 | loss: 0.06179 | val_0_mae: 0.23797 |  0:00:00s\n",
      "epoch 19 | loss: 0.06206 | val_0_mae: 0.22592 |  0:00:00s\n",
      "epoch 20 | loss: 0.06104 | val_0_mae: 0.21977 |  0:00:00s\n",
      "epoch 21 | loss: 0.06057 | val_0_mae: 0.21485 |  0:00:00s\n",
      "epoch 22 | loss: 0.06042 | val_0_mae: 0.21349 |  0:00:00s\n",
      "epoch 23 | loss: 0.06205 | val_0_mae: 0.21612 |  0:00:01s\n",
      "epoch 24 | loss: 0.06009 | val_0_mae: 0.21633 |  0:00:01s\n",
      "epoch 25 | loss: 0.06036 | val_0_mae: 0.21229 |  0:00:01s\n",
      "epoch 26 | loss: 0.0605  | val_0_mae: 0.21051 |  0:00:01s\n",
      "epoch 27 | loss: 0.05943 | val_0_mae: 0.20754 |  0:00:01s\n",
      "epoch 28 | loss: 0.05949 | val_0_mae: 0.20568 |  0:00:01s\n",
      "epoch 29 | loss: 0.06032 | val_0_mae: 0.2057  |  0:00:01s\n",
      "epoch 30 | loss: 0.06077 | val_0_mae: 0.20667 |  0:00:01s\n",
      "epoch 31 | loss: 0.06052 | val_0_mae: 0.20551 |  0:00:01s\n",
      "epoch 32 | loss: 0.05894 | val_0_mae: 0.20645 |  0:00:01s\n",
      "epoch 33 | loss: 0.05974 | val_0_mae: 0.20578 |  0:00:01s\n",
      "epoch 34 | loss: 0.06002 | val_0_mae: 0.20543 |  0:00:01s\n",
      "epoch 35 | loss: 0.0585  | val_0_mae: 0.20486 |  0:00:01s\n",
      "epoch 36 | loss: 0.05929 | val_0_mae: 0.20563 |  0:00:01s\n",
      "epoch 37 | loss: 0.05988 | val_0_mae: 0.20624 |  0:00:01s\n",
      "epoch 38 | loss: 0.06078 | val_0_mae: 0.20573 |  0:00:01s\n",
      "epoch 39 | loss: 0.06027 | val_0_mae: 0.20378 |  0:00:01s\n",
      "epoch 40 | loss: 0.05922 | val_0_mae: 0.2031  |  0:00:01s\n",
      "epoch 41 | loss: 0.05967 | val_0_mae: 0.2021  |  0:00:01s\n",
      "epoch 42 | loss: 0.05875 | val_0_mae: 0.20251 |  0:00:01s\n",
      "epoch 43 | loss: 0.0591  | val_0_mae: 0.20506 |  0:00:01s\n",
      "epoch 44 | loss: 0.05895 | val_0_mae: 0.20329 |  0:00:01s\n",
      "epoch 45 | loss: 0.05901 | val_0_mae: 0.20435 |  0:00:01s\n",
      "epoch 46 | loss: 0.05894 | val_0_mae: 0.2028  |  0:00:02s\n",
      "epoch 47 | loss: 0.06009 | val_0_mae: 0.20363 |  0:00:02s\n",
      "epoch 48 | loss: 0.05882 | val_0_mae: 0.204   |  0:00:02s\n",
      "epoch 49 | loss: 0.05905 | val_0_mae: 0.20404 |  0:00:02s\n",
      "epoch 50 | loss: 0.05871 | val_0_mae: 0.20154 |  0:00:02s\n",
      "epoch 51 | loss: 0.05948 | val_0_mae: 0.20437 |  0:00:02s\n",
      "epoch 52 | loss: 0.06001 | val_0_mae: 0.20343 |  0:00:02s\n",
      "epoch 53 | loss: 0.05905 | val_0_mae: 0.20337 |  0:00:02s\n",
      "epoch 54 | loss: 0.05952 | val_0_mae: 0.20354 |  0:00:02s\n",
      "epoch 55 | loss: 0.05954 | val_0_mae: 0.20441 |  0:00:02s\n",
      "epoch 56 | loss: 0.06051 | val_0_mae: 0.20266 |  0:00:02s\n",
      "epoch 57 | loss: 0.05861 | val_0_mae: 0.20282 |  0:00:02s\n",
      "epoch 58 | loss: 0.0593  | val_0_mae: 0.20142 |  0:00:02s\n",
      "epoch 59 | loss: 0.05884 | val_0_mae: 0.20104 |  0:00:02s\n",
      "epoch 60 | loss: 0.05841 | val_0_mae: 0.20041 |  0:00:02s\n",
      "epoch 61 | loss: 0.05892 | val_0_mae: 0.20103 |  0:00:02s\n",
      "epoch 62 | loss: 0.05802 | val_0_mae: 0.19981 |  0:00:02s\n",
      "epoch 63 | loss: 0.05897 | val_0_mae: 0.20014 |  0:00:02s\n",
      "epoch 64 | loss: 0.05852 | val_0_mae: 0.20007 |  0:00:02s\n",
      "epoch 65 | loss: 0.05731 | val_0_mae: 0.20077 |  0:00:02s\n",
      "epoch 66 | loss: 0.05941 | val_0_mae: 0.20131 |  0:00:02s\n",
      "epoch 67 | loss: 0.05852 | val_0_mae: 0.20014 |  0:00:02s\n",
      "epoch 68 | loss: 0.05893 | val_0_mae: 0.20082 |  0:00:02s\n",
      "epoch 69 | loss: 0.05901 | val_0_mae: 0.20186 |  0:00:03s\n",
      "epoch 70 | loss: 0.05824 | val_0_mae: 0.20272 |  0:00:03s\n",
      "epoch 71 | loss: 0.05919 | val_0_mae: 0.20156 |  0:00:03s\n",
      "epoch 72 | loss: 0.05811 | val_0_mae: 0.20082 |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 72 with best_epoch = 62 and best_val_0_mae = 0.19981\n",
      "\n",
      "🔁 Fold 5/10\n",
      "▶ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Fine-tuning...\n",
      "epoch 0  | loss: 10.56116| val_0_mae: 1.33075 |  0:00:00s\n",
      "epoch 1  | loss: 1.03649 | val_0_mae: 1.04295 |  0:00:00s\n",
      "epoch 2  | loss: 0.39883 | val_0_mae: 1.04113 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.20555 | val_0_mae: 0.73774 |  0:00:00s\n",
      "epoch 4  | loss: 0.13324 | val_0_mae: 0.45456 |  0:00:00s\n",
      "epoch 5  | loss: 0.11787 | val_0_mae: 0.49885 |  0:00:00s\n",
      "epoch 6  | loss: 0.08715 | val_0_mae: 0.3789  |  0:00:00s\n",
      "epoch 7  | loss: 0.07998 | val_0_mae: 0.37336 |  0:00:00s\n",
      "epoch 8  | loss: 0.07251 | val_0_mae: 0.38126 |  0:00:00s\n",
      "epoch 9  | loss: 0.07005 | val_0_mae: 0.29965 |  0:00:00s\n",
      "epoch 10 | loss: 0.06683 | val_0_mae: 0.27402 |  0:00:00s\n",
      "epoch 11 | loss: 0.0648  | val_0_mae: 0.23675 |  0:00:00s\n",
      "epoch 12 | loss: 0.064   | val_0_mae: 0.2266  |  0:00:00s\n",
      "epoch 13 | loss: 0.06322 | val_0_mae: 0.23718 |  0:00:00s\n",
      "epoch 14 | loss: 0.06436 | val_0_mae: 0.22618 |  0:00:00s\n",
      "epoch 15 | loss: 0.06265 | val_0_mae: 0.22964 |  0:00:00s\n",
      "epoch 16 | loss: 0.06115 | val_0_mae: 0.23788 |  0:00:00s\n",
      "epoch 17 | loss: 0.06196 | val_0_mae: 0.22802 |  0:00:00s\n",
      "epoch 18 | loss: 0.05978 | val_0_mae: 0.22858 |  0:00:00s\n",
      "epoch 19 | loss: 0.06183 | val_0_mae: 0.21601 |  0:00:00s\n",
      "epoch 20 | loss: 0.06036 | val_0_mae: 0.21542 |  0:00:00s\n",
      "epoch 21 | loss: 0.06175 | val_0_mae: 0.22079 |  0:00:00s\n",
      "epoch 22 | loss: 0.06129 | val_0_mae: 0.21889 |  0:00:01s\n",
      "epoch 23 | loss: 0.06025 | val_0_mae: 0.21692 |  0:00:01s\n",
      "epoch 24 | loss: 0.05929 | val_0_mae: 0.21818 |  0:00:01s\n",
      "epoch 25 | loss: 0.06094 | val_0_mae: 0.21382 |  0:00:01s\n",
      "epoch 26 | loss: 0.06017 | val_0_mae: 0.21735 |  0:00:01s\n",
      "epoch 27 | loss: 0.06045 | val_0_mae: 0.21577 |  0:00:01s\n",
      "epoch 28 | loss: 0.05916 | val_0_mae: 0.21388 |  0:00:01s\n",
      "epoch 29 | loss: 0.06059 | val_0_mae: 0.21431 |  0:00:01s\n",
      "epoch 30 | loss: 0.05958 | val_0_mae: 0.21106 |  0:00:01s\n",
      "epoch 31 | loss: 0.05989 | val_0_mae: 0.21246 |  0:00:01s\n",
      "epoch 32 | loss: 0.06028 | val_0_mae: 0.21024 |  0:00:01s\n",
      "epoch 33 | loss: 0.0589  | val_0_mae: 0.20932 |  0:00:01s\n",
      "epoch 34 | loss: 0.05912 | val_0_mae: 0.20793 |  0:00:01s\n",
      "epoch 35 | loss: 0.06023 | val_0_mae: 0.20949 |  0:00:01s\n",
      "epoch 36 | loss: 0.05959 | val_0_mae: 0.20869 |  0:00:01s\n",
      "epoch 37 | loss: 0.05891 | val_0_mae: 0.20653 |  0:00:01s\n",
      "epoch 38 | loss: 0.05893 | val_0_mae: 0.20961 |  0:00:01s\n",
      "epoch 39 | loss: 0.05947 | val_0_mae: 0.20876 |  0:00:01s\n",
      "epoch 40 | loss: 0.05794 | val_0_mae: 0.20786 |  0:00:01s\n",
      "epoch 41 | loss: 0.05941 | val_0_mae: 0.20858 |  0:00:01s\n",
      "epoch 42 | loss: 0.05932 | val_0_mae: 0.20706 |  0:00:01s\n",
      "epoch 43 | loss: 0.05918 | val_0_mae: 0.20887 |  0:00:01s\n",
      "epoch 44 | loss: 0.05951 | val_0_mae: 0.20827 |  0:00:01s\n",
      "epoch 45 | loss: 0.05851 | val_0_mae: 0.20643 |  0:00:02s\n",
      "epoch 46 | loss: 0.05924 | val_0_mae: 0.20825 |  0:00:02s\n",
      "epoch 47 | loss: 0.05959 | val_0_mae: 0.20945 |  0:00:02s\n",
      "epoch 48 | loss: 0.05884 | val_0_mae: 0.20848 |  0:00:02s\n",
      "epoch 49 | loss: 0.05829 | val_0_mae: 0.2107  |  0:00:02s\n",
      "epoch 50 | loss: 0.05884 | val_0_mae: 0.20869 |  0:00:02s\n",
      "epoch 51 | loss: 0.05884 | val_0_mae: 0.21061 |  0:00:02s\n",
      "epoch 52 | loss: 0.05855 | val_0_mae: 0.20734 |  0:00:02s\n",
      "epoch 53 | loss: 0.05795 | val_0_mae: 0.20858 |  0:00:02s\n",
      "epoch 54 | loss: 0.05978 | val_0_mae: 0.20765 |  0:00:02s\n",
      "epoch 55 | loss: 0.05888 | val_0_mae: 0.20994 |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 55 with best_epoch = 45 and best_val_0_mae = 0.20643\n",
      "\n",
      "🔁 Fold 6/10\n",
      "▶ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Fine-tuning...\n",
      "epoch 0  | loss: 5.78926 | val_0_mae: 1.0516  |  0:00:00s\n",
      "epoch 1  | loss: 0.60912 | val_0_mae: 0.54361 |  0:00:00s\n",
      "epoch 2  | loss: 0.23803 | val_0_mae: 0.50783 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.13172 | val_0_mae: 0.34487 |  0:00:00s\n",
      "epoch 4  | loss: 0.09817 | val_0_mae: 0.24505 |  0:00:00s\n",
      "epoch 5  | loss: 0.08438 | val_0_mae: 0.28956 |  0:00:00s\n",
      "epoch 6  | loss: 0.07669 | val_0_mae: 0.33858 |  0:00:00s\n",
      "epoch 7  | loss: 0.07249 | val_0_mae: 0.3186  |  0:00:00s\n",
      "epoch 8  | loss: 0.06981 | val_0_mae: 0.26283 |  0:00:00s\n",
      "epoch 9  | loss: 0.0673  | val_0_mae: 0.21953 |  0:00:00s\n",
      "epoch 10 | loss: 0.06612 | val_0_mae: 0.2103  |  0:00:00s\n",
      "epoch 11 | loss: 0.06582 | val_0_mae: 0.21738 |  0:00:00s\n",
      "epoch 12 | loss: 0.06245 | val_0_mae: 0.22444 |  0:00:00s\n",
      "epoch 13 | loss: 0.06339 | val_0_mae: 0.22816 |  0:00:00s\n",
      "epoch 14 | loss: 0.0619  | val_0_mae: 0.21869 |  0:00:00s\n",
      "epoch 15 | loss: 0.06036 | val_0_mae: 0.20863 |  0:00:00s\n",
      "epoch 16 | loss: 0.06112 | val_0_mae: 0.20516 |  0:00:00s\n",
      "epoch 17 | loss: 0.06044 | val_0_mae: 0.20392 |  0:00:00s\n",
      "epoch 18 | loss: 0.05977 | val_0_mae: 0.204   |  0:00:00s\n",
      "epoch 19 | loss: 0.06033 | val_0_mae: 0.20499 |  0:00:00s\n",
      "epoch 20 | loss: 0.05916 | val_0_mae: 0.20681 |  0:00:00s\n",
      "epoch 21 | loss: 0.0597  | val_0_mae: 0.20328 |  0:00:00s\n",
      "epoch 22 | loss: 0.0605  | val_0_mae: 0.20054 |  0:00:01s\n",
      "epoch 23 | loss: 0.06119 | val_0_mae: 0.19936 |  0:00:01s\n",
      "epoch 24 | loss: 0.06065 | val_0_mae: 0.19897 |  0:00:01s\n",
      "epoch 25 | loss: 0.05954 | val_0_mae: 0.19835 |  0:00:01s\n",
      "epoch 26 | loss: 0.05966 | val_0_mae: 0.19766 |  0:00:01s\n",
      "epoch 27 | loss: 0.05917 | val_0_mae: 0.19877 |  0:00:01s\n",
      "epoch 28 | loss: 0.05906 | val_0_mae: 0.19872 |  0:00:01s\n",
      "epoch 29 | loss: 0.06028 | val_0_mae: 0.1958  |  0:00:01s\n",
      "epoch 30 | loss: 0.06018 | val_0_mae: 0.19417 |  0:00:01s\n",
      "epoch 31 | loss: 0.05973 | val_0_mae: 0.19555 |  0:00:01s\n",
      "epoch 32 | loss: 0.05925 | val_0_mae: 0.19612 |  0:00:01s\n",
      "epoch 33 | loss: 0.05997 | val_0_mae: 0.19746 |  0:00:01s\n",
      "epoch 34 | loss: 0.05877 | val_0_mae: 0.19516 |  0:00:01s\n",
      "epoch 35 | loss: 0.05913 | val_0_mae: 0.19581 |  0:00:01s\n",
      "epoch 36 | loss: 0.0595  | val_0_mae: 0.19735 |  0:00:01s\n",
      "epoch 37 | loss: 0.05901 | val_0_mae: 0.1983  |  0:00:01s\n",
      "epoch 38 | loss: 0.05905 | val_0_mae: 0.19826 |  0:00:01s\n",
      "epoch 39 | loss: 0.05875 | val_0_mae: 0.19706 |  0:00:01s\n",
      "epoch 40 | loss: 0.05872 | val_0_mae: 0.19793 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 40 with best_epoch = 30 and best_val_0_mae = 0.19417\n",
      "\n",
      "🔁 Fold 7/10\n",
      "▶ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Fine-tuning...\n",
      "epoch 0  | loss: 10.64314| val_0_mae: 1.44936 |  0:00:00s\n",
      "epoch 1  | loss: 1.70525 | val_0_mae: 2.17105 |  0:00:00s\n",
      "epoch 2  | loss: 0.64996 | val_0_mae: 2.03985 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.35942 | val_0_mae: 1.17896 |  0:00:00s\n",
      "epoch 4  | loss: 0.18008 | val_0_mae: 0.88454 |  0:00:00s\n",
      "epoch 5  | loss: 0.14692 | val_0_mae: 0.78485 |  0:00:00s\n",
      "epoch 6  | loss: 0.10794 | val_0_mae: 0.65372 |  0:00:00s\n",
      "epoch 7  | loss: 0.10042 | val_0_mae: 0.62188 |  0:00:00s\n",
      "epoch 8  | loss: 0.0879  | val_0_mae: 0.60838 |  0:00:00s\n",
      "epoch 9  | loss: 0.08276 | val_0_mae: 0.53846 |  0:00:00s\n",
      "epoch 10 | loss: 0.08057 | val_0_mae: 0.49264 |  0:00:00s\n",
      "epoch 11 | loss: 0.07249 | val_0_mae: 0.45871 |  0:00:00s\n",
      "epoch 12 | loss: 0.07449 | val_0_mae: 0.37725 |  0:00:00s\n",
      "epoch 13 | loss: 0.07075 | val_0_mae: 0.39097 |  0:00:00s\n",
      "epoch 14 | loss: 0.06576 | val_0_mae: 0.39674 |  0:00:00s\n",
      "epoch 15 | loss: 0.06821 | val_0_mae: 0.36741 |  0:00:00s\n",
      "epoch 16 | loss: 0.06582 | val_0_mae: 0.33916 |  0:00:00s\n",
      "epoch 17 | loss: 0.06456 | val_0_mae: 0.32681 |  0:00:00s\n",
      "epoch 18 | loss: 0.06638 | val_0_mae: 0.30052 |  0:00:00s\n",
      "epoch 19 | loss: 0.06479 | val_0_mae: 0.30613 |  0:00:00s\n",
      "epoch 20 | loss: 0.06459 | val_0_mae: 0.30315 |  0:00:00s\n",
      "epoch 21 | loss: 0.06526 | val_0_mae: 0.28173 |  0:00:00s\n",
      "epoch 22 | loss: 0.06358 | val_0_mae: 0.28177 |  0:00:01s\n",
      "epoch 23 | loss: 0.06263 | val_0_mae: 0.26602 |  0:00:01s\n",
      "epoch 24 | loss: 0.0629  | val_0_mae: 0.27813 |  0:00:01s\n",
      "epoch 25 | loss: 0.06354 | val_0_mae: 0.26701 |  0:00:01s\n",
      "epoch 26 | loss: 0.06274 | val_0_mae: 0.24674 |  0:00:01s\n",
      "epoch 27 | loss: 0.06207 | val_0_mae: 0.25882 |  0:00:01s\n",
      "epoch 28 | loss: 0.0625  | val_0_mae: 0.25749 |  0:00:01s\n",
      "epoch 29 | loss: 0.06141 | val_0_mae: 0.23418 |  0:00:01s\n",
      "epoch 30 | loss: 0.06145 | val_0_mae: 0.24181 |  0:00:01s\n",
      "epoch 31 | loss: 0.0612  | val_0_mae: 0.23178 |  0:00:01s\n",
      "epoch 32 | loss: 0.06042 | val_0_mae: 0.22779 |  0:00:01s\n",
      "epoch 33 | loss: 0.06097 | val_0_mae: 0.23307 |  0:00:01s\n",
      "epoch 34 | loss: 0.06106 | val_0_mae: 0.22941 |  0:00:01s\n",
      "epoch 35 | loss: 0.0607  | val_0_mae: 0.22215 |  0:00:01s\n",
      "epoch 36 | loss: 0.06099 | val_0_mae: 0.23047 |  0:00:01s\n",
      "epoch 37 | loss: 0.06277 | val_0_mae: 0.23028 |  0:00:01s\n",
      "epoch 38 | loss: 0.06222 | val_0_mae: 0.22465 |  0:00:01s\n",
      "epoch 39 | loss: 0.06217 | val_0_mae: 0.22364 |  0:00:01s\n",
      "epoch 40 | loss: 0.06179 | val_0_mae: 0.22602 |  0:00:01s\n",
      "epoch 41 | loss: 0.06097 | val_0_mae: 0.21437 |  0:00:01s\n",
      "epoch 42 | loss: 0.06146 | val_0_mae: 0.217   |  0:00:01s\n",
      "epoch 43 | loss: 0.06265 | val_0_mae: 0.21423 |  0:00:01s\n",
      "epoch 44 | loss: 0.05938 | val_0_mae: 0.20956 |  0:00:01s\n",
      "epoch 45 | loss: 0.06083 | val_0_mae: 0.21695 |  0:00:02s\n",
      "epoch 46 | loss: 0.06067 | val_0_mae: 0.20438 |  0:00:02s\n",
      "epoch 47 | loss: 0.06053 | val_0_mae: 0.20732 |  0:00:02s\n",
      "epoch 48 | loss: 0.06047 | val_0_mae: 0.20499 |  0:00:02s\n",
      "epoch 49 | loss: 0.05973 | val_0_mae: 0.20572 |  0:00:02s\n",
      "epoch 50 | loss: 0.06102 | val_0_mae: 0.20643 |  0:00:02s\n",
      "epoch 51 | loss: 0.05983 | val_0_mae: 0.20615 |  0:00:02s\n",
      "epoch 52 | loss: 0.0587  | val_0_mae: 0.20234 |  0:00:02s\n",
      "epoch 53 | loss: 0.05919 | val_0_mae: 0.20952 |  0:00:02s\n",
      "epoch 54 | loss: 0.06063 | val_0_mae: 0.20462 |  0:00:02s\n",
      "epoch 55 | loss: 0.06001 | val_0_mae: 0.20276 |  0:00:02s\n",
      "epoch 56 | loss: 0.06074 | val_0_mae: 0.20656 |  0:00:02s\n",
      "epoch 57 | loss: 0.06041 | val_0_mae: 0.20233 |  0:00:02s\n",
      "epoch 58 | loss: 0.06162 | val_0_mae: 0.20169 |  0:00:02s\n",
      "epoch 59 | loss: 0.06069 | val_0_mae: 0.2063  |  0:00:02s\n",
      "epoch 60 | loss: 0.0593  | val_0_mae: 0.20282 |  0:00:02s\n",
      "epoch 61 | loss: 0.05917 | val_0_mae: 0.20304 |  0:00:02s\n",
      "epoch 62 | loss: 0.06018 | val_0_mae: 0.20404 |  0:00:02s\n",
      "epoch 63 | loss: 0.06027 | val_0_mae: 0.20057 |  0:00:02s\n",
      "epoch 64 | loss: 0.0593  | val_0_mae: 0.20558 |  0:00:02s\n",
      "epoch 65 | loss: 0.05984 | val_0_mae: 0.20026 |  0:00:02s\n",
      "epoch 66 | loss: 0.05962 | val_0_mae: 0.20359 |  0:00:02s\n",
      "epoch 67 | loss: 0.05878 | val_0_mae: 0.20125 |  0:00:02s\n",
      "epoch 68 | loss: 0.06033 | val_0_mae: 0.20296 |  0:00:03s\n",
      "epoch 69 | loss: 0.05878 | val_0_mae: 0.20196 |  0:00:03s\n",
      "epoch 70 | loss: 0.05886 | val_0_mae: 0.20296 |  0:00:03s\n",
      "epoch 71 | loss: 0.05969 | val_0_mae: 0.2011  |  0:00:03s\n",
      "epoch 72 | loss: 0.05879 | val_0_mae: 0.20238 |  0:00:03s\n",
      "epoch 73 | loss: 0.05948 | val_0_mae: 0.2005  |  0:00:03s\n",
      "epoch 74 | loss: 0.05998 | val_0_mae: 0.20279 |  0:00:03s\n",
      "epoch 75 | loss: 0.05853 | val_0_mae: 0.20065 |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 75 with best_epoch = 65 and best_val_0_mae = 0.20026\n",
      "\n",
      "🔁 Fold 8/10\n",
      "▶ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Fine-tuning...\n",
      "epoch 0  | loss: 21.63481| val_0_mae: 1.0781  |  0:00:00s\n",
      "epoch 1  | loss: 3.09279 | val_0_mae: 0.77832 |  0:00:00s\n",
      "epoch 2  | loss: 0.96579 | val_0_mae: 0.94559 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.52167 | val_0_mae: 0.75642 |  0:00:00s\n",
      "epoch 4  | loss: 0.23327 | val_0_mae: 0.6464  |  0:00:00s\n",
      "epoch 5  | loss: 0.1796  | val_0_mae: 0.56044 |  0:00:00s\n",
      "epoch 6  | loss: 0.13187 | val_0_mae: 0.39991 |  0:00:00s\n",
      "epoch 7  | loss: 0.11129 | val_0_mae: 0.43814 |  0:00:00s\n",
      "epoch 8  | loss: 0.09356 | val_0_mae: 0.29762 |  0:00:00s\n",
      "epoch 9  | loss: 0.08548 | val_0_mae: 0.30705 |  0:00:00s\n",
      "epoch 10 | loss: 0.07912 | val_0_mae: 0.27592 |  0:00:00s\n",
      "epoch 11 | loss: 0.07355 | val_0_mae: 0.27796 |  0:00:00s\n",
      "epoch 12 | loss: 0.07419 | val_0_mae: 0.22442 |  0:00:00s\n",
      "epoch 13 | loss: 0.07264 | val_0_mae: 0.24702 |  0:00:00s\n",
      "epoch 14 | loss: 0.06857 | val_0_mae: 0.23935 |  0:00:00s\n",
      "epoch 15 | loss: 0.0684  | val_0_mae: 0.22726 |  0:00:00s\n",
      "epoch 16 | loss: 0.06633 | val_0_mae: 0.22324 |  0:00:00s\n",
      "epoch 17 | loss: 0.06619 | val_0_mae: 0.22367 |  0:00:00s\n",
      "epoch 18 | loss: 0.06401 | val_0_mae: 0.22105 |  0:00:00s\n",
      "epoch 19 | loss: 0.06501 | val_0_mae: 0.22392 |  0:00:00s\n",
      "epoch 20 | loss: 0.06286 | val_0_mae: 0.21656 |  0:00:01s\n",
      "epoch 21 | loss: 0.06362 | val_0_mae: 0.2223  |  0:00:01s\n",
      "epoch 22 | loss: 0.06365 | val_0_mae: 0.21972 |  0:00:01s\n",
      "epoch 23 | loss: 0.06369 | val_0_mae: 0.22095 |  0:00:01s\n",
      "epoch 24 | loss: 0.064   | val_0_mae: 0.21451 |  0:00:01s\n",
      "epoch 25 | loss: 0.06217 | val_0_mae: 0.21696 |  0:00:01s\n",
      "epoch 26 | loss: 0.06216 | val_0_mae: 0.21035 |  0:00:01s\n",
      "epoch 27 | loss: 0.06365 | val_0_mae: 0.20968 |  0:00:01s\n",
      "epoch 28 | loss: 0.06262 | val_0_mae: 0.2114  |  0:00:01s\n",
      "epoch 29 | loss: 0.06192 | val_0_mae: 0.2061  |  0:00:01s\n",
      "epoch 30 | loss: 0.06332 | val_0_mae: 0.2061  |  0:00:01s\n",
      "epoch 31 | loss: 0.0619  | val_0_mae: 0.20951 |  0:00:01s\n",
      "epoch 32 | loss: 0.06212 | val_0_mae: 0.20965 |  0:00:01s\n",
      "epoch 33 | loss: 0.0613  | val_0_mae: 0.20755 |  0:00:01s\n",
      "epoch 34 | loss: 0.06204 | val_0_mae: 0.2108  |  0:00:01s\n",
      "epoch 35 | loss: 0.06176 | val_0_mae: 0.20993 |  0:00:01s\n",
      "epoch 36 | loss: 0.06042 | val_0_mae: 0.20989 |  0:00:01s\n",
      "epoch 37 | loss: 0.06091 | val_0_mae: 0.21247 |  0:00:01s\n",
      "epoch 38 | loss: 0.06069 | val_0_mae: 0.21398 |  0:00:01s\n",
      "epoch 39 | loss: 0.05942 | val_0_mae: 0.21241 |  0:00:01s\n",
      "epoch 40 | loss: 0.06154 | val_0_mae: 0.21204 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 40 with best_epoch = 30 and best_val_0_mae = 0.2061\n",
      "\n",
      "🔁 Fold 9/10\n",
      "▶ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Fine-tuning...\n",
      "epoch 0  | loss: 15.7881 | val_0_mae: 1.90869 |  0:00:00s\n",
      "epoch 1  | loss: 2.05157 | val_0_mae: 0.89672 |  0:00:00s\n",
      "epoch 2  | loss: 0.55127 | val_0_mae: 0.66361 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.27287 | val_0_mae: 0.50429 |  0:00:00s\n",
      "epoch 4  | loss: 0.14112 | val_0_mae: 0.44656 |  0:00:00s\n",
      "epoch 5  | loss: 0.10665 | val_0_mae: 0.34357 |  0:00:00s\n",
      "epoch 6  | loss: 0.08628 | val_0_mae: 0.23959 |  0:00:00s\n",
      "epoch 7  | loss: 0.07728 | val_0_mae: 0.25752 |  0:00:00s\n",
      "epoch 8  | loss: 0.07184 | val_0_mae: 0.24558 |  0:00:00s\n",
      "epoch 9  | loss: 0.07168 | val_0_mae: 0.22289 |  0:00:00s\n",
      "epoch 10 | loss: 0.06634 | val_0_mae: 0.2394  |  0:00:00s\n",
      "epoch 11 | loss: 0.06364 | val_0_mae: 0.24462 |  0:00:00s\n",
      "epoch 12 | loss: 0.06418 | val_0_mae: 0.22706 |  0:00:00s\n",
      "epoch 13 | loss: 0.06242 | val_0_mae: 0.21251 |  0:00:00s\n",
      "epoch 14 | loss: 0.06142 | val_0_mae: 0.21476 |  0:00:00s\n",
      "epoch 15 | loss: 0.06157 | val_0_mae: 0.21294 |  0:00:00s\n",
      "epoch 16 | loss: 0.06135 | val_0_mae: 0.21452 |  0:00:00s\n",
      "epoch 17 | loss: 0.06097 | val_0_mae: 0.2132  |  0:00:00s\n",
      "epoch 18 | loss: 0.06089 | val_0_mae: 0.20859 |  0:00:00s\n",
      "epoch 19 | loss: 0.06202 | val_0_mae: 0.21147 |  0:00:00s\n",
      "epoch 20 | loss: 0.0598  | val_0_mae: 0.20648 |  0:00:00s\n",
      "epoch 21 | loss: 0.06096 | val_0_mae: 0.20648 |  0:00:01s\n",
      "epoch 22 | loss: 0.06071 | val_0_mae: 0.2096  |  0:00:01s\n",
      "epoch 23 | loss: 0.05963 | val_0_mae: 0.20049 |  0:00:01s\n",
      "epoch 24 | loss: 0.05995 | val_0_mae: 0.2066  |  0:00:01s\n",
      "epoch 25 | loss: 0.06121 | val_0_mae: 0.20376 |  0:00:01s\n",
      "epoch 26 | loss: 0.06001 | val_0_mae: 0.20081 |  0:00:01s\n",
      "epoch 27 | loss: 0.06022 | val_0_mae: 0.20696 |  0:00:01s\n",
      "epoch 28 | loss: 0.06004 | val_0_mae: 0.203   |  0:00:01s\n",
      "epoch 29 | loss: 0.06098 | val_0_mae: 0.20242 |  0:00:01s\n",
      "epoch 30 | loss: 0.05933 | val_0_mae: 0.20264 |  0:00:01s\n",
      "epoch 31 | loss: 0.06025 | val_0_mae: 0.19979 |  0:00:01s\n",
      "epoch 32 | loss: 0.05934 | val_0_mae: 0.20147 |  0:00:01s\n",
      "epoch 33 | loss: 0.05968 | val_0_mae: 0.20101 |  0:00:01s\n",
      "epoch 34 | loss: 0.05938 | val_0_mae: 0.20197 |  0:00:01s\n",
      "epoch 35 | loss: 0.05913 | val_0_mae: 0.20401 |  0:00:01s\n",
      "epoch 36 | loss: 0.05945 | val_0_mae: 0.20222 |  0:00:01s\n",
      "epoch 37 | loss: 0.0598  | val_0_mae: 0.20464 |  0:00:01s\n",
      "epoch 38 | loss: 0.05856 | val_0_mae: 0.20199 |  0:00:01s\n",
      "epoch 39 | loss: 0.05915 | val_0_mae: 0.20466 |  0:00:01s\n",
      "epoch 40 | loss: 0.05858 | val_0_mae: 0.20394 |  0:00:01s\n",
      "epoch 41 | loss: 0.05902 | val_0_mae: 0.20331 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 31 and best_val_0_mae = 0.19979\n",
      "\n",
      "🔁 Fold 10/10\n",
      "▶ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▶ Fine-tuning...\n",
      "epoch 0  | loss: 3.45799 | val_0_mae: 0.52669 |  0:00:00s\n",
      "epoch 1  | loss: 0.41193 | val_0_mae: 0.43101 |  0:00:00s\n",
      "epoch 2  | loss: 0.20684 | val_0_mae: 0.47479 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.12299 | val_0_mae: 0.28763 |  0:00:00s\n",
      "epoch 4  | loss: 0.10577 | val_0_mae: 0.25542 |  0:00:00s\n",
      "epoch 5  | loss: 0.08835 | val_0_mae: 0.24428 |  0:00:00s\n",
      "epoch 6  | loss: 0.07895 | val_0_mae: 0.23209 |  0:00:00s\n",
      "epoch 7  | loss: 0.07471 | val_0_mae: 0.22482 |  0:00:00s\n",
      "epoch 8  | loss: 0.06727 | val_0_mae: 0.21613 |  0:00:00s\n",
      "epoch 9  | loss: 0.06767 | val_0_mae: 0.22197 |  0:00:00s\n",
      "epoch 10 | loss: 0.06579 | val_0_mae: 0.22368 |  0:00:00s\n",
      "epoch 11 | loss: 0.06378 | val_0_mae: 0.21551 |  0:00:00s\n",
      "epoch 12 | loss: 0.06067 | val_0_mae: 0.21389 |  0:00:00s\n",
      "epoch 13 | loss: 0.06246 | val_0_mae: 0.21541 |  0:00:00s\n",
      "epoch 14 | loss: 0.06144 | val_0_mae: 0.2126  |  0:00:00s\n",
      "epoch 15 | loss: 0.06078 | val_0_mae: 0.21452 |  0:00:00s\n",
      "epoch 16 | loss: 0.06194 | val_0_mae: 0.21255 |  0:00:00s\n",
      "epoch 17 | loss: 0.05932 | val_0_mae: 0.21313 |  0:00:00s\n",
      "epoch 18 | loss: 0.0602  | val_0_mae: 0.21003 |  0:00:00s\n",
      "epoch 19 | loss: 0.06151 | val_0_mae: 0.21084 |  0:00:00s\n",
      "epoch 20 | loss: 0.05956 | val_0_mae: 0.21354 |  0:00:00s\n",
      "epoch 21 | loss: 0.05957 | val_0_mae: 0.21208 |  0:00:01s\n",
      "epoch 22 | loss: 0.06013 | val_0_mae: 0.20933 |  0:00:01s\n",
      "epoch 23 | loss: 0.06001 | val_0_mae: 0.21072 |  0:00:01s\n",
      "epoch 24 | loss: 0.0602  | val_0_mae: 0.21243 |  0:00:01s\n",
      "epoch 25 | loss: 0.05923 | val_0_mae: 0.21115 |  0:00:01s\n",
      "epoch 26 | loss: 0.0594  | val_0_mae: 0.21165 |  0:00:01s\n",
      "epoch 27 | loss: 0.0586  | val_0_mae: 0.2096  |  0:00:01s\n",
      "epoch 28 | loss: 0.05831 | val_0_mae: 0.21137 |  0:00:01s\n",
      "epoch 29 | loss: 0.05827 | val_0_mae: 0.21122 |  0:00:01s\n",
      "epoch 30 | loss: 0.05883 | val_0_mae: 0.20926 |  0:00:01s\n",
      "epoch 31 | loss: 0.05965 | val_0_mae: 0.21252 |  0:00:01s\n",
      "epoch 32 | loss: 0.05898 | val_0_mae: 0.21271 |  0:00:01s\n",
      "epoch 33 | loss: 0.05737 | val_0_mae: 0.21332 |  0:00:01s\n",
      "epoch 34 | loss: 0.05821 | val_0_mae: 0.21074 |  0:00:01s\n",
      "epoch 35 | loss: 0.0583  | val_0_mae: 0.21278 |  0:00:01s\n",
      "epoch 36 | loss: 0.05904 | val_0_mae: 0.21217 |  0:00:01s\n",
      "epoch 37 | loss: 0.05865 | val_0_mae: 0.21326 |  0:00:01s\n",
      "epoch 38 | loss: 0.05922 | val_0_mae: 0.21352 |  0:00:01s\n",
      "epoch 39 | loss: 0.05792 | val_0_mae: 0.21429 |  0:00:01s\n",
      "epoch 40 | loss: 0.05839 | val_0_mae: 0.21265 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 40 with best_epoch = 30 and best_val_0_mae = 0.20926\n",
      "\n",
      "✅ 모든 fold 모델 학습 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "# 타겟 지정\n",
    "target = train['성공확률']  \n",
    "X = train[features]\n",
    "y = target\n",
    "\n",
    "# KFold 설정\n",
    "N_FOLDS = 10\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "models = [] # 모델 저장 리스트\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\n🔁 Fold {fold+1}/{N_FOLDS}\")\n",
    "    \n",
    "    X_train = X.iloc[train_idx].values\n",
    "    y_train = y.iloc[train_idx].values.reshape(-1, 1)\n",
    "    \n",
    "    X_valid = X.iloc[valid_idx].values\n",
    "    y_valid = y.iloc[valid_idx].values.reshape(-1, 1)\n",
    "    \n",
    "    # 비지도 사전학습\n",
    "    print(\"▶ Pretraining...\")\n",
    "\n",
    "    pretrainer = TabNetPretrainer(\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        seed=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    pretrainer.fit(\n",
    "        X_train=X_train,\n",
    "        max_epochs=100,\n",
    "        batch_size=512,\n",
    "        virtual_batch_size=64\n",
    "    )\n",
    "\n",
    "\n",
    "    # 지도 학습 \n",
    "    print(\"▶ Fine-tuning...\")\n",
    "    model = TabNetRegressor(\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        seed=42,\n",
    "        verbose=1,       \n",
    "        optimizer_fn=torch.optim.AdamW,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        from_unsupervised=pretrainer,\n",
    "        eval_metric=['mae'],\n",
    "        max_epochs=100,\n",
    "        patience=10,\n",
    "    )\n",
    "\n",
    "    # 모델을 메모리에 저장\n",
    "    models.append(model)\n",
    "    cv_scores.append(model.best_cost)\n",
    "\n",
    "print(\"\\n✅ 모든 fold 모델 학습 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict with fold 1\n",
      "Predict with fold 2\n",
      "Predict with fold 3\n",
      "Predict with fold 4\n",
      "Predict with fold 5\n",
      "Predict with fold 6\n",
      "Predict with fold 7\n",
      "Predict with fold 8\n",
      "Predict with fold 9\n",
      "Predict with fold 10\n"
     ]
    }
   ],
   "source": [
    "# 저장된 모델들로 예측\n",
    "predictions_list = []\n",
    "\n",
    "for fold, model in enumerate(models):\n",
    "    print(f\"Predict with fold {fold+1}\")\n",
    "    preds = model.predict(test[features].values)\n",
    "    predictions_list.append(preds)\n",
    "\n",
    "# 평균 예측\n",
    "final_predictions = np.mean(predictions_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['성공확률'] = final_predictions\n",
    "sample_submission.to_csv('./baseline_submission5.csv', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Feature  Importance\n",
      "5             상장여부    0.516250\n",
      "4             인수여부    0.244577\n",
      "11              연차    0.136009\n",
      "3             직원 수    0.022742\n",
      "1               분야    0.021302\n",
      "10       기업가치(백억원)    0.020834\n",
      "9   SNS 팔로워 수(백만명)    0.016876\n",
      "8          연매출(억원)    0.014887\n",
      "2             투자단계    0.003031\n",
      "6         고객수(백만명)    0.001531\n",
      "7        총 투자금(억원)    0.000988\n",
      "0               국가    0.000975\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 모델에서 중요도 추출 (예: 첫 번째 fold 모델 사용)\n",
    "importances = models[0].feature_importances_\n",
    "\n",
    "# 컬럼명과 매핑\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# 중요도 기준 정렬\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# 결과 출력\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
