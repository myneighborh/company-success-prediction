{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.tab_model import TabNetRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('../data/train.csv')\n",
    "test = pd.read_csv('../data/test.csv')\n",
    "sample_submission = pd.read_csv('../data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ID Ïª¨Îüº Î∂ÑÎ¶¨\n",
    "train = train.drop(columns=['ID'], axis = 1)\n",
    "test = test.drop(columns=['ID'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ÏÑ§Î¶ΩÏó∞ÎèÑ</th>\n",
       "      <th>Íµ≠Í∞Ä</th>\n",
       "      <th>Î∂ÑÏïº</th>\n",
       "      <th>Ìà¨ÏûêÎã®Í≥Ñ</th>\n",
       "      <th>ÏßÅÏõê Ïàò</th>\n",
       "      <th>Ïù∏ÏàòÏó¨Î∂Ä</th>\n",
       "      <th>ÏÉÅÏû•Ïó¨Î∂Ä</th>\n",
       "      <th>Í≥†Í∞ùÏàò(Î∞±ÎßåÎ™Ö)</th>\n",
       "      <th>Ï¥ù Ìà¨ÏûêÍ∏à(ÏñµÏõê)</th>\n",
       "      <th>Ïó∞Îß§Ï∂ú(ÏñµÏõê)</th>\n",
       "      <th>SNS ÌåîÎ°úÏõå Ïàò(Î∞±ÎßåÎ™Ö)</th>\n",
       "      <th>Í∏∞ÏóÖÍ∞ÄÏπò(Î∞±ÏñµÏõê)</th>\n",
       "      <th>ÏÑ±Í≥µÌôïÎ•†</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2009</td>\n",
       "      <td>CT005</td>\n",
       "      <td>Ïù¥Ïª§Î®∏Ïä§</td>\n",
       "      <td>Series A</td>\n",
       "      <td>4126.0</td>\n",
       "      <td>No</td>\n",
       "      <td>No</td>\n",
       "      <td>56.0</td>\n",
       "      <td>3365.0</td>\n",
       "      <td>4764.0</td>\n",
       "      <td>4.71</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2023</td>\n",
       "      <td>CT006</td>\n",
       "      <td>ÌïÄÌÖåÌÅ¨</td>\n",
       "      <td>Seed</td>\n",
       "      <td>4167.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>No</td>\n",
       "      <td>80.0</td>\n",
       "      <td>4069.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2500-3500</td>\n",
       "      <td>0.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2018</td>\n",
       "      <td>CT007</td>\n",
       "      <td>Í∏∞Ïà†</td>\n",
       "      <td>Series A</td>\n",
       "      <td>3132.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>54.0</td>\n",
       "      <td>6453.0</td>\n",
       "      <td>12141.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>3500-4500</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2016</td>\n",
       "      <td>CT006</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Seed</td>\n",
       "      <td>3245.0</td>\n",
       "      <td>Yes</td>\n",
       "      <td>Yes</td>\n",
       "      <td>NaN</td>\n",
       "      <td>665.0</td>\n",
       "      <td>10547.0</td>\n",
       "      <td>2.97</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2020</td>\n",
       "      <td>CT002</td>\n",
       "      <td>ÏóêÎìÄÌÖåÌÅ¨</td>\n",
       "      <td>Seed</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>No</td>\n",
       "      <td>Yes</td>\n",
       "      <td>94.0</td>\n",
       "      <td>829.0</td>\n",
       "      <td>9810.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>1500-2500</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ÏÑ§Î¶ΩÏó∞ÎèÑ     Íµ≠Í∞Ä    Î∂ÑÏïº      Ìà¨ÏûêÎã®Í≥Ñ    ÏßÅÏõê Ïàò Ïù∏ÏàòÏó¨Î∂Ä ÏÉÅÏû•Ïó¨Î∂Ä  Í≥†Í∞ùÏàò(Î∞±ÎßåÎ™Ö)  Ï¥ù Ìà¨ÏûêÍ∏à(ÏñµÏõê)  \\\n",
       "0  2009  CT005  Ïù¥Ïª§Î®∏Ïä§  Series A  4126.0   No   No      56.0     3365.0   \n",
       "1  2023  CT006   ÌïÄÌÖåÌÅ¨      Seed  4167.0  Yes   No      80.0     4069.0   \n",
       "2  2018  CT007    Í∏∞Ïà†  Series A  3132.0  Yes  Yes      54.0     6453.0   \n",
       "3  2016  CT006   NaN      Seed  3245.0  Yes  Yes       NaN      665.0   \n",
       "4  2020  CT002  ÏóêÎìÄÌÖåÌÅ¨      Seed  1969.0   No  Yes      94.0      829.0   \n",
       "\n",
       "   Ïó∞Îß§Ï∂ú(ÏñµÏõê)  SNS ÌåîÎ°úÏõå Ïàò(Î∞±ÎßåÎ™Ö)  Í∏∞ÏóÖÍ∞ÄÏπò(Î∞±ÏñµÏõê)  ÏÑ±Í≥µÌôïÎ•†  \n",
       "0   4764.0            4.71        NaN   0.3  \n",
       "1    279.0            1.00  2500-3500   0.8  \n",
       "2  12141.0            4.00  3500-4500   0.5  \n",
       "3  10547.0            2.97        NaN   0.7  \n",
       "4   9810.0            1.00  1500-2500   0.1  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_valuation(val):\n",
    "    if pd.isna(val):\n",
    "        return None\n",
    "    if 'Ïù¥ÏÉÅ' in val:\n",
    "        # '6000Ïù¥ÏÉÅ' ‚Üí 6000\n",
    "        return int(re.sub('[^0-9]', '', val))\n",
    "    elif '-' in val:\n",
    "        # '2500-3500' ‚Üí ÌèâÍ∑†Í∞í Í≥ÑÏÇ∞\n",
    "        low, high = map(int, val.split('-'))\n",
    "        return (low + high) / 2\n",
    "    else:\n",
    "        # Ïà´ÏûêÎ°ú Î≥ÄÌôò Í∞ÄÎä•Ìïú Í≤ΩÏö∞\n",
    "        try:\n",
    "            return float(val)\n",
    "        except:\n",
    "            return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ÏÑ§Î¶ΩÏó∞ÎèÑ -> Ïó∞Ï∞®Î°ú Î≥ÄÍ≤Ω\n",
    "current_year = 2025\n",
    "\n",
    "train['Ïó∞Ï∞®'] = current_year - train['ÏÑ§Î¶ΩÏó∞ÎèÑ']\n",
    "test['Ïó∞Ï∞®'] = current_year - test['ÏÑ§Î¶ΩÏó∞ÎèÑ']\n",
    "\n",
    "# ÏÑ§Î¶ΩÏó∞ÎèÑ Ï†úÍ±∞\n",
    "train.drop(columns = ['ÏÑ§Î¶ΩÏó∞ÎèÑ'], inplace = True)\n",
    "test.drop(columns = ['ÏÑ§Î¶ΩÏó∞ÎèÑ'], inplace = True)\n",
    "\n",
    "category_features = ['Íµ≠Í∞Ä','Î∂ÑÏïº']\n",
    "numeric_features = ['Ïó∞Ï∞®', 'Ìà¨ÏûêÎã®Í≥Ñ', 'ÏßÅÏõê Ïàò','Í≥†Í∞ùÏàò(Î∞±ÎßåÎ™Ö)','Ï¥ù Ìà¨ÏûêÍ∏à(ÏñµÏõê)','Ïó∞Îß§Ï∂ú(ÏñµÏõê)','SNS ÌåîÎ°úÏõå Ïàò(Î∞±ÎßåÎ™Ö)', 'Í∏∞ÏóÖÍ∞ÄÏπò(Î∞±ÏñµÏõê)']\n",
    "bool_features = ['Ïù∏ÏàòÏó¨Î∂Ä','ÏÉÅÏû•Ïó¨Î∂Ä']\n",
    "\n",
    "# Ìà¨ÏûêÎã®Í≥Ñ ÏàúÏÑúÎ•º Ïà´ÏûêÎ°ú Îß§Ìïë\n",
    "investment_stage_map = {\n",
    "    'Seed': 0,\n",
    "    'Series A': 1,\n",
    "    'Series B': 2,\n",
    "    'Series C': 3,\n",
    "    'IPO': 4,\n",
    "    'Missing': -1\n",
    "}\n",
    "\n",
    "# Í≤∞Ï∏°Ïπò Î®ºÏ†Ä Ï≤òÎ¶¨ ÌõÑ Îß§Ìïë\n",
    "train['Ìà¨ÏûêÎã®Í≥Ñ'] = train['Ìà¨ÏûêÎã®Í≥Ñ'].fillna('Missing').map(investment_stage_map)\n",
    "test['Ìà¨ÏûêÎã®Í≥Ñ'] = test['Ìà¨ÏûêÎã®Í≥Ñ'].fillna('Missing').map(investment_stage_map)\n",
    "\n",
    "# Í∏∞ÏóÖÍ∞ÄÏπò Î≥ÄÌôò\n",
    "train['Í∏∞ÏóÖÍ∞ÄÏπò(Î∞±ÏñµÏõê)'] = train['Í∏∞ÏóÖÍ∞ÄÏπò(Î∞±ÏñµÏõê)'].apply(clean_valuation)\n",
    "test['Í∏∞ÏóÖÍ∞ÄÏπò(Î∞±ÏñµÏõê)'] = test['Í∏∞ÏóÖÍ∞ÄÏπò(Î∞±ÏñµÏõê)'].apply(clean_valuation)\n",
    "\n",
    "# LabelEncoder Í∞ùÏ≤¥Î•º Í∞Å Î≤îÏ£ºÌòï featureÎ≥ÑÎ°ú Îî∞Î°ú Ï†ÄÏû•ÌïòÏó¨ ÏÇ¨Ïö©\n",
    "encoders = {}\n",
    "\n",
    "# Î≤îÏ£ºÌòï Îç∞Ïù¥ÌÑ∞Î•º encoding\n",
    "for feature in category_features:\n",
    "    encoders[feature] = LabelEncoder()\n",
    "    train[feature] = train[feature].fillna('Missing')\n",
    "    test[feature] = test[feature].fillna('Missing')\n",
    "    train[feature] = encoders[feature].fit_transform(train[feature])\n",
    "    test[feature] = encoders[feature].transform(test[feature])\n",
    "\n",
    "# Î∂àÎ¶¨Ïñ∏ Í∞íÏùÑ 0Í≥º 1Î°ú Î≥ÄÌôò ('Yes' ‚Üí 1, 'No' ‚Üí 0 ÏúºÎ°ú Î≥ÄÌôò)\n",
    "bool_map = {'Yes': 1, 'No': 0}\n",
    "\n",
    "for feature in bool_features:\n",
    "    train[feature] = train[feature].map(bool_map)\n",
    "    test[feature] = test[feature].map(bool_map)\n",
    "\n",
    "# ÏàòÏπòÌòï Î≥ÄÏàò Í≤∞Ï∏°ÏπòÎ•º Ï§ëÍ∞ÑÍ∞íÏúºÎ°ú ÎåÄÏ≤¥\n",
    "for feature in numeric_features:\n",
    "    median_value = train[feature].median()\n",
    "    train[feature] = train[feature].fillna(median_value)\n",
    "    test[feature] = test[feature].fillna(median_value)\n",
    "\n",
    "# TabNetÏö© Î≤îÏ£ºÌòï Î≥ÄÏàò Ïù∏Îç±Ïä§(cat_idxs) Î∞è Ï∞®Ïõê(cat_dims) ÏÑ§Ï†ï\n",
    "features = [col for col in train.columns if col != 'ÏÑ±Í≥µÌôïÎ•†']\n",
    "cat_idxs = [features.index(col) for col in category_features]\n",
    "cat_dims = [train[col].max() + 1 for col in category_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Íµ≠Í∞Ä</th>\n",
       "      <th>Î∂ÑÏïº</th>\n",
       "      <th>Ìà¨ÏûêÎã®Í≥Ñ</th>\n",
       "      <th>ÏßÅÏõê Ïàò</th>\n",
       "      <th>Ïù∏ÏàòÏó¨Î∂Ä</th>\n",
       "      <th>ÏÉÅÏû•Ïó¨Î∂Ä</th>\n",
       "      <th>Í≥†Í∞ùÏàò(Î∞±ÎßåÎ™Ö)</th>\n",
       "      <th>Ï¥ù Ìà¨ÏûêÍ∏à(ÏñµÏõê)</th>\n",
       "      <th>Ïó∞Îß§Ï∂ú(ÏñµÏõê)</th>\n",
       "      <th>SNS ÌåîÎ°úÏõå Ïàò(Î∞±ÎßåÎ™Ö)</th>\n",
       "      <th>Í∏∞ÏóÖÍ∞ÄÏπò(Î∞±ÏñµÏõê)</th>\n",
       "      <th>ÏÑ±Í≥µÌôïÎ•†</th>\n",
       "      <th>Ïó∞Ï∞®</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>7</td>\n",
       "      <td>1</td>\n",
       "      <td>4126.0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>56.000000</td>\n",
       "      <td>3365.0</td>\n",
       "      <td>4764.0</td>\n",
       "      <td>4.71</td>\n",
       "      <td>4049.984157</td>\n",
       "      <td>0.3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>4167.0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>80.000000</td>\n",
       "      <td>4069.0</td>\n",
       "      <td>279.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>3000.000000</td>\n",
       "      <td>0.8</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>3132.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>54.000000</td>\n",
       "      <td>6453.0</td>\n",
       "      <td>12141.0</td>\n",
       "      <td>4.00</td>\n",
       "      <td>4000.000000</td>\n",
       "      <td>0.5</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3245.0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>49.214332</td>\n",
       "      <td>665.0</td>\n",
       "      <td>10547.0</td>\n",
       "      <td>2.97</td>\n",
       "      <td>4049.984157</td>\n",
       "      <td>0.7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>1969.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>94.000000</td>\n",
       "      <td>829.0</td>\n",
       "      <td>9810.0</td>\n",
       "      <td>1.00</td>\n",
       "      <td>2000.000000</td>\n",
       "      <td>0.1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Íµ≠Í∞Ä  Î∂ÑÏïº  Ìà¨ÏûêÎã®Í≥Ñ    ÏßÅÏõê Ïàò  Ïù∏ÏàòÏó¨Î∂Ä  ÏÉÅÏû•Ïó¨Î∂Ä   Í≥†Í∞ùÏàò(Î∞±ÎßåÎ™Ö)  Ï¥ù Ìà¨ÏûêÍ∏à(ÏñµÏõê)  Ïó∞Îß§Ï∂ú(ÏñµÏõê)  \\\n",
       "0   4   7     1  4126.0     0     0  56.000000     3365.0   4764.0   \n",
       "1   5   9     0  4167.0     1     0  80.000000     4069.0    279.0   \n",
       "2   6   3     1  3132.0     1     1  54.000000     6453.0  12141.0   \n",
       "3   5   1     0  3245.0     1     1  49.214332      665.0  10547.0   \n",
       "4   1   6     0  1969.0     0     1  94.000000      829.0   9810.0   \n",
       "\n",
       "   SNS ÌåîÎ°úÏõå Ïàò(Î∞±ÎßåÎ™Ö)    Í∏∞ÏóÖÍ∞ÄÏπò(Î∞±ÏñµÏõê)  ÏÑ±Í≥µÌôïÎ•†  Ïó∞Ï∞®  \n",
       "0            4.71  4049.984157   0.3  16  \n",
       "1            1.00  3000.000000   0.8   2  \n",
       "2            4.00  4000.000000   0.5   7  \n",
       "3            2.97  4049.984157   0.7   9  \n",
       "4            1.00  2000.000000   0.1   5  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üîÅ Fold 1/10\n",
      "‚ñ∂ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Fine-tuning...\n",
      "epoch 0  | loss: 9.75046 | val_0_mae: 1.88839 |  0:00:00s\n",
      "epoch 1  | loss: 1.61759 | val_0_mae: 2.18384 |  0:00:00s\n",
      "epoch 2  | loss: 0.70409 | val_0_mae: 1.40964 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.31501 | val_0_mae: 0.94061 |  0:00:00s\n",
      "epoch 4  | loss: 0.19432 | val_0_mae: 1.16273 |  0:00:00s\n",
      "epoch 5  | loss: 0.12828 | val_0_mae: 1.22528 |  0:00:00s\n",
      "epoch 6  | loss: 0.10042 | val_0_mae: 0.93654 |  0:00:00s\n",
      "epoch 7  | loss: 0.09095 | val_0_mae: 0.84326 |  0:00:00s\n",
      "epoch 8  | loss: 0.07859 | val_0_mae: 0.74689 |  0:00:00s\n",
      "epoch 9  | loss: 0.0702  | val_0_mae: 0.64778 |  0:00:00s\n",
      "epoch 10 | loss: 0.06966 | val_0_mae: 0.61959 |  0:00:00s\n",
      "epoch 11 | loss: 0.06756 | val_0_mae: 0.53817 |  0:00:00s\n",
      "epoch 12 | loss: 0.06546 | val_0_mae: 0.44731 |  0:00:00s\n",
      "epoch 13 | loss: 0.06788 | val_0_mae: 0.44165 |  0:00:00s\n",
      "epoch 14 | loss: 0.06518 | val_0_mae: 0.41787 |  0:00:00s\n",
      "epoch 15 | loss: 0.06392 | val_0_mae: 0.38005 |  0:00:00s\n",
      "epoch 16 | loss: 0.06155 | val_0_mae: 0.36391 |  0:00:00s\n",
      "epoch 17 | loss: 0.06265 | val_0_mae: 0.34463 |  0:00:00s\n",
      "epoch 18 | loss: 0.06145 | val_0_mae: 0.33018 |  0:00:00s\n",
      "epoch 19 | loss: 0.06156 | val_0_mae: 0.30239 |  0:00:00s\n",
      "epoch 20 | loss: 0.06124 | val_0_mae: 0.28173 |  0:00:00s\n",
      "epoch 21 | loss: 0.06219 | val_0_mae: 0.28897 |  0:00:01s\n",
      "epoch 22 | loss: 0.05984 | val_0_mae: 0.28182 |  0:00:01s\n",
      "epoch 23 | loss: 0.06028 | val_0_mae: 0.26983 |  0:00:01s\n",
      "epoch 24 | loss: 0.06058 | val_0_mae: 0.2558  |  0:00:01s\n",
      "epoch 25 | loss: 0.06096 | val_0_mae: 0.25805 |  0:00:01s\n",
      "epoch 26 | loss: 0.06124 | val_0_mae: 0.26055 |  0:00:01s\n",
      "epoch 27 | loss: 0.06081 | val_0_mae: 0.24875 |  0:00:01s\n",
      "epoch 28 | loss: 0.06147 | val_0_mae: 0.24526 |  0:00:01s\n",
      "epoch 29 | loss: 0.06128 | val_0_mae: 0.24257 |  0:00:01s\n",
      "epoch 30 | loss: 0.06042 | val_0_mae: 0.23908 |  0:00:01s\n",
      "epoch 31 | loss: 0.06126 | val_0_mae: 0.23203 |  0:00:01s\n",
      "epoch 32 | loss: 0.06023 | val_0_mae: 0.2403  |  0:00:01s\n",
      "epoch 33 | loss: 0.05971 | val_0_mae: 0.22449 |  0:00:01s\n",
      "epoch 34 | loss: 0.06057 | val_0_mae: 0.23017 |  0:00:01s\n",
      "epoch 35 | loss: 0.05947 | val_0_mae: 0.21926 |  0:00:01s\n",
      "epoch 36 | loss: 0.05996 | val_0_mae: 0.21989 |  0:00:01s\n",
      "epoch 37 | loss: 0.06022 | val_0_mae: 0.21875 |  0:00:01s\n",
      "epoch 38 | loss: 0.06061 | val_0_mae: 0.2207  |  0:00:01s\n",
      "epoch 39 | loss: 0.05946 | val_0_mae: 0.21662 |  0:00:01s\n",
      "epoch 40 | loss: 0.05953 | val_0_mae: 0.21254 |  0:00:01s\n",
      "epoch 41 | loss: 0.06018 | val_0_mae: 0.21003 |  0:00:01s\n",
      "epoch 42 | loss: 0.05945 | val_0_mae: 0.21448 |  0:00:01s\n",
      "epoch 43 | loss: 0.05968 | val_0_mae: 0.20498 |  0:00:01s\n",
      "epoch 44 | loss: 0.05876 | val_0_mae: 0.20587 |  0:00:02s\n",
      "epoch 45 | loss: 0.0593  | val_0_mae: 0.20559 |  0:00:02s\n",
      "epoch 46 | loss: 0.05975 | val_0_mae: 0.20363 |  0:00:02s\n",
      "epoch 47 | loss: 0.05928 | val_0_mae: 0.20663 |  0:00:02s\n",
      "epoch 48 | loss: 0.05962 | val_0_mae: 0.20384 |  0:00:02s\n",
      "epoch 49 | loss: 0.05941 | val_0_mae: 0.20474 |  0:00:02s\n",
      "epoch 50 | loss: 0.05931 | val_0_mae: 0.19934 |  0:00:02s\n",
      "epoch 51 | loss: 0.06035 | val_0_mae: 0.20511 |  0:00:02s\n",
      "epoch 52 | loss: 0.05925 | val_0_mae: 0.1997  |  0:00:02s\n",
      "epoch 53 | loss: 0.05919 | val_0_mae: 0.20374 |  0:00:02s\n",
      "epoch 54 | loss: 0.05917 | val_0_mae: 0.20284 |  0:00:02s\n",
      "epoch 55 | loss: 0.05919 | val_0_mae: 0.20381 |  0:00:02s\n",
      "epoch 56 | loss: 0.05892 | val_0_mae: 0.19957 |  0:00:02s\n",
      "epoch 57 | loss: 0.05888 | val_0_mae: 0.206   |  0:00:02s\n",
      "epoch 58 | loss: 0.05957 | val_0_mae: 0.201   |  0:00:02s\n",
      "epoch 59 | loss: 0.06094 | val_0_mae: 0.20276 |  0:00:02s\n",
      "epoch 60 | loss: 0.05951 | val_0_mae: 0.19908 |  0:00:02s\n",
      "epoch 61 | loss: 0.05846 | val_0_mae: 0.20033 |  0:00:02s\n",
      "epoch 62 | loss: 0.05913 | val_0_mae: 0.19987 |  0:00:02s\n",
      "epoch 63 | loss: 0.05873 | val_0_mae: 0.19936 |  0:00:02s\n",
      "epoch 64 | loss: 0.05794 | val_0_mae: 0.19969 |  0:00:02s\n",
      "epoch 65 | loss: 0.05939 | val_0_mae: 0.20089 |  0:00:02s\n",
      "epoch 66 | loss: 0.05865 | val_0_mae: 0.20074 |  0:00:02s\n",
      "epoch 67 | loss: 0.05831 | val_0_mae: 0.20096 |  0:00:02s\n",
      "epoch 68 | loss: 0.0593  | val_0_mae: 0.20082 |  0:00:03s\n",
      "epoch 69 | loss: 0.05896 | val_0_mae: 0.20031 |  0:00:03s\n",
      "epoch 70 | loss: 0.05872 | val_0_mae: 0.19955 |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 70 with best_epoch = 60 and best_val_0_mae = 0.19908\n",
      "\n",
      "üîÅ Fold 2/10\n",
      "‚ñ∂ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Fine-tuning...\n",
      "epoch 0  | loss: 4.18775 | val_0_mae: 0.98624 |  0:00:00s\n",
      "epoch 1  | loss: 0.82364 | val_0_mae: 0.67322 |  0:00:00s\n",
      "epoch 2  | loss: 0.38889 | val_0_mae: 0.76104 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.16747 | val_0_mae: 0.39123 |  0:00:00s\n",
      "epoch 4  | loss: 0.12381 | val_0_mae: 0.43226 |  0:00:00s\n",
      "epoch 5  | loss: 0.10231 | val_0_mae: 0.30735 |  0:00:00s\n",
      "epoch 6  | loss: 0.08949 | val_0_mae: 0.30989 |  0:00:00s\n",
      "epoch 7  | loss: 0.07885 | val_0_mae: 0.27362 |  0:00:00s\n",
      "epoch 8  | loss: 0.07322 | val_0_mae: 0.24294 |  0:00:00s\n",
      "epoch 9  | loss: 0.06925 | val_0_mae: 0.23566 |  0:00:00s\n",
      "epoch 10 | loss: 0.06934 | val_0_mae: 0.23672 |  0:00:00s\n",
      "epoch 11 | loss: 0.06684 | val_0_mae: 0.23535 |  0:00:00s\n",
      "epoch 12 | loss: 0.06552 | val_0_mae: 0.23731 |  0:00:00s\n",
      "epoch 13 | loss: 0.06328 | val_0_mae: 0.21843 |  0:00:00s\n",
      "epoch 14 | loss: 0.0639  | val_0_mae: 0.21734 |  0:00:00s\n",
      "epoch 15 | loss: 0.06229 | val_0_mae: 0.22649 |  0:00:00s\n",
      "epoch 16 | loss: 0.06276 | val_0_mae: 0.21554 |  0:00:00s\n",
      "epoch 17 | loss: 0.06201 | val_0_mae: 0.21416 |  0:00:00s\n",
      "epoch 18 | loss: 0.06348 | val_0_mae: 0.21856 |  0:00:00s\n",
      "epoch 19 | loss: 0.05984 | val_0_mae: 0.22202 |  0:00:00s\n",
      "epoch 20 | loss: 0.06182 | val_0_mae: 0.21898 |  0:00:00s\n",
      "epoch 21 | loss: 0.0609  | val_0_mae: 0.21442 |  0:00:00s\n",
      "epoch 22 | loss: 0.06164 | val_0_mae: 0.21596 |  0:00:01s\n",
      "epoch 23 | loss: 0.06107 | val_0_mae: 0.21559 |  0:00:01s\n",
      "epoch 24 | loss: 0.05903 | val_0_mae: 0.2131  |  0:00:01s\n",
      "epoch 25 | loss: 0.05976 | val_0_mae: 0.21195 |  0:00:01s\n",
      "epoch 26 | loss: 0.0588  | val_0_mae: 0.21283 |  0:00:01s\n",
      "epoch 27 | loss: 0.0597  | val_0_mae: 0.21291 |  0:00:01s\n",
      "epoch 28 | loss: 0.05933 | val_0_mae: 0.21376 |  0:00:01s\n",
      "epoch 29 | loss: 0.05973 | val_0_mae: 0.21617 |  0:00:01s\n",
      "epoch 30 | loss: 0.06032 | val_0_mae: 0.21421 |  0:00:01s\n",
      "epoch 31 | loss: 0.06083 | val_0_mae: 0.21365 |  0:00:01s\n",
      "epoch 32 | loss: 0.05877 | val_0_mae: 0.21345 |  0:00:01s\n",
      "epoch 33 | loss: 0.05849 | val_0_mae: 0.21167 |  0:00:01s\n",
      "epoch 34 | loss: 0.05863 | val_0_mae: 0.20912 |  0:00:01s\n",
      "epoch 35 | loss: 0.05946 | val_0_mae: 0.21451 |  0:00:01s\n",
      "epoch 36 | loss: 0.05932 | val_0_mae: 0.20851 |  0:00:01s\n",
      "epoch 37 | loss: 0.05892 | val_0_mae: 0.21261 |  0:00:01s\n",
      "epoch 38 | loss: 0.0586  | val_0_mae: 0.21532 |  0:00:01s\n",
      "epoch 39 | loss: 0.05762 | val_0_mae: 0.21228 |  0:00:01s\n",
      "epoch 40 | loss: 0.05833 | val_0_mae: 0.213   |  0:00:01s\n",
      "epoch 41 | loss: 0.05891 | val_0_mae: 0.21877 |  0:00:01s\n",
      "epoch 42 | loss: 0.05976 | val_0_mae: 0.21676 |  0:00:01s\n",
      "epoch 43 | loss: 0.05964 | val_0_mae: 0.21583 |  0:00:01s\n",
      "epoch 44 | loss: 0.05888 | val_0_mae: 0.21454 |  0:00:01s\n",
      "epoch 45 | loss: 0.05923 | val_0_mae: 0.21638 |  0:00:01s\n",
      "epoch 46 | loss: 0.05932 | val_0_mae: 0.21494 |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 46 with best_epoch = 36 and best_val_0_mae = 0.20851\n",
      "\n",
      "üîÅ Fold 3/10\n",
      "‚ñ∂ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Fine-tuning...\n",
      "epoch 0  | loss: 11.10798| val_0_mae: 1.25535 |  0:00:00s\n",
      "epoch 1  | loss: 1.65661 | val_0_mae: 0.58924 |  0:00:00s\n",
      "epoch 2  | loss: 0.43875 | val_0_mae: 0.62683 |  0:00:00s\n",
      "epoch 3  | loss: 0.22724 | val_0_mae: 0.79719 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 4  | loss: 0.14166 | val_0_mae: 0.59279 |  0:00:00s\n",
      "epoch 5  | loss: 0.10119 | val_0_mae: 0.48227 |  0:00:00s\n",
      "epoch 6  | loss: 0.09552 | val_0_mae: 0.34663 |  0:00:00s\n",
      "epoch 7  | loss: 0.07253 | val_0_mae: 0.28092 |  0:00:00s\n",
      "epoch 8  | loss: 0.07659 | val_0_mae: 0.27806 |  0:00:00s\n",
      "epoch 9  | loss: 0.06976 | val_0_mae: 0.23746 |  0:00:00s\n",
      "epoch 10 | loss: 0.06929 | val_0_mae: 0.2537  |  0:00:00s\n",
      "epoch 11 | loss: 0.06581 | val_0_mae: 0.22823 |  0:00:00s\n",
      "epoch 12 | loss: 0.06478 | val_0_mae: 0.23972 |  0:00:00s\n",
      "epoch 13 | loss: 0.06292 | val_0_mae: 0.22152 |  0:00:00s\n",
      "epoch 14 | loss: 0.06051 | val_0_mae: 0.23276 |  0:00:00s\n",
      "epoch 15 | loss: 0.06177 | val_0_mae: 0.21741 |  0:00:00s\n",
      "epoch 16 | loss: 0.06063 | val_0_mae: 0.22927 |  0:00:00s\n",
      "epoch 17 | loss: 0.06196 | val_0_mae: 0.21485 |  0:00:00s\n",
      "epoch 18 | loss: 0.06055 | val_0_mae: 0.22251 |  0:00:00s\n",
      "epoch 19 | loss: 0.0611  | val_0_mae: 0.21204 |  0:00:00s\n",
      "epoch 20 | loss: 0.05966 | val_0_mae: 0.21603 |  0:00:00s\n",
      "epoch 21 | loss: 0.05849 | val_0_mae: 0.21848 |  0:00:01s\n",
      "epoch 22 | loss: 0.0603  | val_0_mae: 0.21489 |  0:00:01s\n",
      "epoch 23 | loss: 0.0591  | val_0_mae: 0.21404 |  0:00:01s\n",
      "epoch 24 | loss: 0.05922 | val_0_mae: 0.21669 |  0:00:01s\n",
      "epoch 25 | loss: 0.0599  | val_0_mae: 0.20967 |  0:00:01s\n",
      "epoch 26 | loss: 0.05828 | val_0_mae: 0.2127  |  0:00:01s\n",
      "epoch 27 | loss: 0.05849 | val_0_mae: 0.21062 |  0:00:01s\n",
      "epoch 28 | loss: 0.05864 | val_0_mae: 0.21313 |  0:00:01s\n",
      "epoch 29 | loss: 0.05818 | val_0_mae: 0.20966 |  0:00:01s\n",
      "epoch 30 | loss: 0.05802 | val_0_mae: 0.21592 |  0:00:01s\n",
      "epoch 31 | loss: 0.05869 | val_0_mae: 0.21106 |  0:00:01s\n",
      "epoch 32 | loss: 0.0607  | val_0_mae: 0.21293 |  0:00:01s\n",
      "epoch 33 | loss: 0.05873 | val_0_mae: 0.21283 |  0:00:01s\n",
      "epoch 34 | loss: 0.05896 | val_0_mae: 0.2125  |  0:00:01s\n",
      "epoch 35 | loss: 0.0586  | val_0_mae: 0.22091 |  0:00:01s\n",
      "epoch 36 | loss: 0.06012 | val_0_mae: 0.21317 |  0:00:01s\n",
      "epoch 37 | loss: 0.05912 | val_0_mae: 0.21339 |  0:00:01s\n",
      "epoch 38 | loss: 0.05836 | val_0_mae: 0.21321 |  0:00:01s\n",
      "epoch 39 | loss: 0.05811 | val_0_mae: 0.21447 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 39 with best_epoch = 29 and best_val_0_mae = 0.20966\n",
      "\n",
      "üîÅ Fold 4/10\n",
      "‚ñ∂ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Fine-tuning...\n",
      "epoch 0  | loss: 4.54647 | val_0_mae: 0.6813  |  0:00:00s\n",
      "epoch 1  | loss: 0.63129 | val_0_mae: 0.48261 |  0:00:00s\n",
      "epoch 2  | loss: 0.23344 | val_0_mae: 0.90957 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.14892 | val_0_mae: 0.61017 |  0:00:00s\n",
      "epoch 4  | loss: 0.10334 | val_0_mae: 0.445   |  0:00:00s\n",
      "epoch 5  | loss: 0.09102 | val_0_mae: 0.36818 |  0:00:00s\n",
      "epoch 6  | loss: 0.08382 | val_0_mae: 0.46324 |  0:00:00s\n",
      "epoch 7  | loss: 0.07508 | val_0_mae: 0.38998 |  0:00:00s\n",
      "epoch 8  | loss: 0.07185 | val_0_mae: 0.28595 |  0:00:00s\n",
      "epoch 9  | loss: 0.06895 | val_0_mae: 0.25508 |  0:00:00s\n",
      "epoch 10 | loss: 0.06794 | val_0_mae: 0.22236 |  0:00:00s\n",
      "epoch 11 | loss: 0.0691  | val_0_mae: 0.22382 |  0:00:00s\n",
      "epoch 12 | loss: 0.06603 | val_0_mae: 0.24949 |  0:00:00s\n",
      "epoch 13 | loss: 0.06621 | val_0_mae: 0.26609 |  0:00:00s\n",
      "epoch 14 | loss: 0.06416 | val_0_mae: 0.2389  |  0:00:00s\n",
      "epoch 15 | loss: 0.0649  | val_0_mae: 0.23658 |  0:00:00s\n",
      "epoch 16 | loss: 0.0643  | val_0_mae: 0.22636 |  0:00:00s\n",
      "epoch 17 | loss: 0.06344 | val_0_mae: 0.22872 |  0:00:00s\n",
      "epoch 18 | loss: 0.06179 | val_0_mae: 0.23797 |  0:00:00s\n",
      "epoch 19 | loss: 0.06206 | val_0_mae: 0.22592 |  0:00:00s\n",
      "epoch 20 | loss: 0.06104 | val_0_mae: 0.21977 |  0:00:00s\n",
      "epoch 21 | loss: 0.06057 | val_0_mae: 0.21485 |  0:00:00s\n",
      "epoch 22 | loss: 0.06042 | val_0_mae: 0.21349 |  0:00:00s\n",
      "epoch 23 | loss: 0.06205 | val_0_mae: 0.21612 |  0:00:01s\n",
      "epoch 24 | loss: 0.06009 | val_0_mae: 0.21633 |  0:00:01s\n",
      "epoch 25 | loss: 0.06036 | val_0_mae: 0.21229 |  0:00:01s\n",
      "epoch 26 | loss: 0.0605  | val_0_mae: 0.21051 |  0:00:01s\n",
      "epoch 27 | loss: 0.05943 | val_0_mae: 0.20754 |  0:00:01s\n",
      "epoch 28 | loss: 0.05949 | val_0_mae: 0.20568 |  0:00:01s\n",
      "epoch 29 | loss: 0.06032 | val_0_mae: 0.2057  |  0:00:01s\n",
      "epoch 30 | loss: 0.06077 | val_0_mae: 0.20667 |  0:00:01s\n",
      "epoch 31 | loss: 0.06052 | val_0_mae: 0.20551 |  0:00:01s\n",
      "epoch 32 | loss: 0.05894 | val_0_mae: 0.20645 |  0:00:01s\n",
      "epoch 33 | loss: 0.05974 | val_0_mae: 0.20578 |  0:00:01s\n",
      "epoch 34 | loss: 0.06002 | val_0_mae: 0.20543 |  0:00:01s\n",
      "epoch 35 | loss: 0.0585  | val_0_mae: 0.20486 |  0:00:01s\n",
      "epoch 36 | loss: 0.05929 | val_0_mae: 0.20563 |  0:00:01s\n",
      "epoch 37 | loss: 0.05988 | val_0_mae: 0.20624 |  0:00:01s\n",
      "epoch 38 | loss: 0.06078 | val_0_mae: 0.20573 |  0:00:01s\n",
      "epoch 39 | loss: 0.06027 | val_0_mae: 0.20378 |  0:00:01s\n",
      "epoch 40 | loss: 0.05922 | val_0_mae: 0.2031  |  0:00:01s\n",
      "epoch 41 | loss: 0.05967 | val_0_mae: 0.2021  |  0:00:01s\n",
      "epoch 42 | loss: 0.05875 | val_0_mae: 0.20251 |  0:00:01s\n",
      "epoch 43 | loss: 0.0591  | val_0_mae: 0.20506 |  0:00:01s\n",
      "epoch 44 | loss: 0.05895 | val_0_mae: 0.20329 |  0:00:01s\n",
      "epoch 45 | loss: 0.05901 | val_0_mae: 0.20435 |  0:00:01s\n",
      "epoch 46 | loss: 0.05894 | val_0_mae: 0.2028  |  0:00:02s\n",
      "epoch 47 | loss: 0.06009 | val_0_mae: 0.20363 |  0:00:02s\n",
      "epoch 48 | loss: 0.05882 | val_0_mae: 0.204   |  0:00:02s\n",
      "epoch 49 | loss: 0.05905 | val_0_mae: 0.20404 |  0:00:02s\n",
      "epoch 50 | loss: 0.05871 | val_0_mae: 0.20154 |  0:00:02s\n",
      "epoch 51 | loss: 0.05948 | val_0_mae: 0.20437 |  0:00:02s\n",
      "epoch 52 | loss: 0.06001 | val_0_mae: 0.20343 |  0:00:02s\n",
      "epoch 53 | loss: 0.05905 | val_0_mae: 0.20337 |  0:00:02s\n",
      "epoch 54 | loss: 0.05952 | val_0_mae: 0.20354 |  0:00:02s\n",
      "epoch 55 | loss: 0.05954 | val_0_mae: 0.20441 |  0:00:02s\n",
      "epoch 56 | loss: 0.06051 | val_0_mae: 0.20266 |  0:00:02s\n",
      "epoch 57 | loss: 0.05861 | val_0_mae: 0.20282 |  0:00:02s\n",
      "epoch 58 | loss: 0.0593  | val_0_mae: 0.20142 |  0:00:02s\n",
      "epoch 59 | loss: 0.05884 | val_0_mae: 0.20104 |  0:00:02s\n",
      "epoch 60 | loss: 0.05841 | val_0_mae: 0.20041 |  0:00:02s\n",
      "epoch 61 | loss: 0.05892 | val_0_mae: 0.20103 |  0:00:02s\n",
      "epoch 62 | loss: 0.05802 | val_0_mae: 0.19981 |  0:00:02s\n",
      "epoch 63 | loss: 0.05897 | val_0_mae: 0.20014 |  0:00:02s\n",
      "epoch 64 | loss: 0.05852 | val_0_mae: 0.20007 |  0:00:02s\n",
      "epoch 65 | loss: 0.05731 | val_0_mae: 0.20077 |  0:00:02s\n",
      "epoch 66 | loss: 0.05941 | val_0_mae: 0.20131 |  0:00:02s\n",
      "epoch 67 | loss: 0.05852 | val_0_mae: 0.20014 |  0:00:02s\n",
      "epoch 68 | loss: 0.05893 | val_0_mae: 0.20082 |  0:00:02s\n",
      "epoch 69 | loss: 0.05901 | val_0_mae: 0.20186 |  0:00:03s\n",
      "epoch 70 | loss: 0.05824 | val_0_mae: 0.20272 |  0:00:03s\n",
      "epoch 71 | loss: 0.05919 | val_0_mae: 0.20156 |  0:00:03s\n",
      "epoch 72 | loss: 0.05811 | val_0_mae: 0.20082 |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 72 with best_epoch = 62 and best_val_0_mae = 0.19981\n",
      "\n",
      "üîÅ Fold 5/10\n",
      "‚ñ∂ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Fine-tuning...\n",
      "epoch 0  | loss: 10.56116| val_0_mae: 1.33075 |  0:00:00s\n",
      "epoch 1  | loss: 1.03649 | val_0_mae: 1.04295 |  0:00:00s\n",
      "epoch 2  | loss: 0.39883 | val_0_mae: 1.04113 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.20555 | val_0_mae: 0.73774 |  0:00:00s\n",
      "epoch 4  | loss: 0.13324 | val_0_mae: 0.45456 |  0:00:00s\n",
      "epoch 5  | loss: 0.11787 | val_0_mae: 0.49885 |  0:00:00s\n",
      "epoch 6  | loss: 0.08715 | val_0_mae: 0.3789  |  0:00:00s\n",
      "epoch 7  | loss: 0.07998 | val_0_mae: 0.37336 |  0:00:00s\n",
      "epoch 8  | loss: 0.07251 | val_0_mae: 0.38126 |  0:00:00s\n",
      "epoch 9  | loss: 0.07005 | val_0_mae: 0.29965 |  0:00:00s\n",
      "epoch 10 | loss: 0.06683 | val_0_mae: 0.27402 |  0:00:00s\n",
      "epoch 11 | loss: 0.0648  | val_0_mae: 0.23675 |  0:00:00s\n",
      "epoch 12 | loss: 0.064   | val_0_mae: 0.2266  |  0:00:00s\n",
      "epoch 13 | loss: 0.06322 | val_0_mae: 0.23718 |  0:00:00s\n",
      "epoch 14 | loss: 0.06436 | val_0_mae: 0.22618 |  0:00:00s\n",
      "epoch 15 | loss: 0.06265 | val_0_mae: 0.22964 |  0:00:00s\n",
      "epoch 16 | loss: 0.06115 | val_0_mae: 0.23788 |  0:00:00s\n",
      "epoch 17 | loss: 0.06196 | val_0_mae: 0.22802 |  0:00:00s\n",
      "epoch 18 | loss: 0.05978 | val_0_mae: 0.22858 |  0:00:00s\n",
      "epoch 19 | loss: 0.06183 | val_0_mae: 0.21601 |  0:00:00s\n",
      "epoch 20 | loss: 0.06036 | val_0_mae: 0.21542 |  0:00:00s\n",
      "epoch 21 | loss: 0.06175 | val_0_mae: 0.22079 |  0:00:00s\n",
      "epoch 22 | loss: 0.06129 | val_0_mae: 0.21889 |  0:00:01s\n",
      "epoch 23 | loss: 0.06025 | val_0_mae: 0.21692 |  0:00:01s\n",
      "epoch 24 | loss: 0.05929 | val_0_mae: 0.21818 |  0:00:01s\n",
      "epoch 25 | loss: 0.06094 | val_0_mae: 0.21382 |  0:00:01s\n",
      "epoch 26 | loss: 0.06017 | val_0_mae: 0.21735 |  0:00:01s\n",
      "epoch 27 | loss: 0.06045 | val_0_mae: 0.21577 |  0:00:01s\n",
      "epoch 28 | loss: 0.05916 | val_0_mae: 0.21388 |  0:00:01s\n",
      "epoch 29 | loss: 0.06059 | val_0_mae: 0.21431 |  0:00:01s\n",
      "epoch 30 | loss: 0.05958 | val_0_mae: 0.21106 |  0:00:01s\n",
      "epoch 31 | loss: 0.05989 | val_0_mae: 0.21246 |  0:00:01s\n",
      "epoch 32 | loss: 0.06028 | val_0_mae: 0.21024 |  0:00:01s\n",
      "epoch 33 | loss: 0.0589  | val_0_mae: 0.20932 |  0:00:01s\n",
      "epoch 34 | loss: 0.05912 | val_0_mae: 0.20793 |  0:00:01s\n",
      "epoch 35 | loss: 0.06023 | val_0_mae: 0.20949 |  0:00:01s\n",
      "epoch 36 | loss: 0.05959 | val_0_mae: 0.20869 |  0:00:01s\n",
      "epoch 37 | loss: 0.05891 | val_0_mae: 0.20653 |  0:00:01s\n",
      "epoch 38 | loss: 0.05893 | val_0_mae: 0.20961 |  0:00:01s\n",
      "epoch 39 | loss: 0.05947 | val_0_mae: 0.20876 |  0:00:01s\n",
      "epoch 40 | loss: 0.05794 | val_0_mae: 0.20786 |  0:00:01s\n",
      "epoch 41 | loss: 0.05941 | val_0_mae: 0.20858 |  0:00:01s\n",
      "epoch 42 | loss: 0.05932 | val_0_mae: 0.20706 |  0:00:01s\n",
      "epoch 43 | loss: 0.05918 | val_0_mae: 0.20887 |  0:00:01s\n",
      "epoch 44 | loss: 0.05951 | val_0_mae: 0.20827 |  0:00:01s\n",
      "epoch 45 | loss: 0.05851 | val_0_mae: 0.20643 |  0:00:02s\n",
      "epoch 46 | loss: 0.05924 | val_0_mae: 0.20825 |  0:00:02s\n",
      "epoch 47 | loss: 0.05959 | val_0_mae: 0.20945 |  0:00:02s\n",
      "epoch 48 | loss: 0.05884 | val_0_mae: 0.20848 |  0:00:02s\n",
      "epoch 49 | loss: 0.05829 | val_0_mae: 0.2107  |  0:00:02s\n",
      "epoch 50 | loss: 0.05884 | val_0_mae: 0.20869 |  0:00:02s\n",
      "epoch 51 | loss: 0.05884 | val_0_mae: 0.21061 |  0:00:02s\n",
      "epoch 52 | loss: 0.05855 | val_0_mae: 0.20734 |  0:00:02s\n",
      "epoch 53 | loss: 0.05795 | val_0_mae: 0.20858 |  0:00:02s\n",
      "epoch 54 | loss: 0.05978 | val_0_mae: 0.20765 |  0:00:02s\n",
      "epoch 55 | loss: 0.05888 | val_0_mae: 0.20994 |  0:00:02s\n",
      "\n",
      "Early stopping occurred at epoch 55 with best_epoch = 45 and best_val_0_mae = 0.20643\n",
      "\n",
      "üîÅ Fold 6/10\n",
      "‚ñ∂ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Fine-tuning...\n",
      "epoch 0  | loss: 5.78926 | val_0_mae: 1.0516  |  0:00:00s\n",
      "epoch 1  | loss: 0.60912 | val_0_mae: 0.54361 |  0:00:00s\n",
      "epoch 2  | loss: 0.23803 | val_0_mae: 0.50783 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.13172 | val_0_mae: 0.34487 |  0:00:00s\n",
      "epoch 4  | loss: 0.09817 | val_0_mae: 0.24505 |  0:00:00s\n",
      "epoch 5  | loss: 0.08438 | val_0_mae: 0.28956 |  0:00:00s\n",
      "epoch 6  | loss: 0.07669 | val_0_mae: 0.33858 |  0:00:00s\n",
      "epoch 7  | loss: 0.07249 | val_0_mae: 0.3186  |  0:00:00s\n",
      "epoch 8  | loss: 0.06981 | val_0_mae: 0.26283 |  0:00:00s\n",
      "epoch 9  | loss: 0.0673  | val_0_mae: 0.21953 |  0:00:00s\n",
      "epoch 10 | loss: 0.06612 | val_0_mae: 0.2103  |  0:00:00s\n",
      "epoch 11 | loss: 0.06582 | val_0_mae: 0.21738 |  0:00:00s\n",
      "epoch 12 | loss: 0.06245 | val_0_mae: 0.22444 |  0:00:00s\n",
      "epoch 13 | loss: 0.06339 | val_0_mae: 0.22816 |  0:00:00s\n",
      "epoch 14 | loss: 0.0619  | val_0_mae: 0.21869 |  0:00:00s\n",
      "epoch 15 | loss: 0.06036 | val_0_mae: 0.20863 |  0:00:00s\n",
      "epoch 16 | loss: 0.06112 | val_0_mae: 0.20516 |  0:00:00s\n",
      "epoch 17 | loss: 0.06044 | val_0_mae: 0.20392 |  0:00:00s\n",
      "epoch 18 | loss: 0.05977 | val_0_mae: 0.204   |  0:00:00s\n",
      "epoch 19 | loss: 0.06033 | val_0_mae: 0.20499 |  0:00:00s\n",
      "epoch 20 | loss: 0.05916 | val_0_mae: 0.20681 |  0:00:00s\n",
      "epoch 21 | loss: 0.0597  | val_0_mae: 0.20328 |  0:00:00s\n",
      "epoch 22 | loss: 0.0605  | val_0_mae: 0.20054 |  0:00:01s\n",
      "epoch 23 | loss: 0.06119 | val_0_mae: 0.19936 |  0:00:01s\n",
      "epoch 24 | loss: 0.06065 | val_0_mae: 0.19897 |  0:00:01s\n",
      "epoch 25 | loss: 0.05954 | val_0_mae: 0.19835 |  0:00:01s\n",
      "epoch 26 | loss: 0.05966 | val_0_mae: 0.19766 |  0:00:01s\n",
      "epoch 27 | loss: 0.05917 | val_0_mae: 0.19877 |  0:00:01s\n",
      "epoch 28 | loss: 0.05906 | val_0_mae: 0.19872 |  0:00:01s\n",
      "epoch 29 | loss: 0.06028 | val_0_mae: 0.1958  |  0:00:01s\n",
      "epoch 30 | loss: 0.06018 | val_0_mae: 0.19417 |  0:00:01s\n",
      "epoch 31 | loss: 0.05973 | val_0_mae: 0.19555 |  0:00:01s\n",
      "epoch 32 | loss: 0.05925 | val_0_mae: 0.19612 |  0:00:01s\n",
      "epoch 33 | loss: 0.05997 | val_0_mae: 0.19746 |  0:00:01s\n",
      "epoch 34 | loss: 0.05877 | val_0_mae: 0.19516 |  0:00:01s\n",
      "epoch 35 | loss: 0.05913 | val_0_mae: 0.19581 |  0:00:01s\n",
      "epoch 36 | loss: 0.0595  | val_0_mae: 0.19735 |  0:00:01s\n",
      "epoch 37 | loss: 0.05901 | val_0_mae: 0.1983  |  0:00:01s\n",
      "epoch 38 | loss: 0.05905 | val_0_mae: 0.19826 |  0:00:01s\n",
      "epoch 39 | loss: 0.05875 | val_0_mae: 0.19706 |  0:00:01s\n",
      "epoch 40 | loss: 0.05872 | val_0_mae: 0.19793 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 40 with best_epoch = 30 and best_val_0_mae = 0.19417\n",
      "\n",
      "üîÅ Fold 7/10\n",
      "‚ñ∂ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Fine-tuning...\n",
      "epoch 0  | loss: 10.64314| val_0_mae: 1.44936 |  0:00:00s\n",
      "epoch 1  | loss: 1.70525 | val_0_mae: 2.17105 |  0:00:00s\n",
      "epoch 2  | loss: 0.64996 | val_0_mae: 2.03985 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.35942 | val_0_mae: 1.17896 |  0:00:00s\n",
      "epoch 4  | loss: 0.18008 | val_0_mae: 0.88454 |  0:00:00s\n",
      "epoch 5  | loss: 0.14692 | val_0_mae: 0.78485 |  0:00:00s\n",
      "epoch 6  | loss: 0.10794 | val_0_mae: 0.65372 |  0:00:00s\n",
      "epoch 7  | loss: 0.10042 | val_0_mae: 0.62188 |  0:00:00s\n",
      "epoch 8  | loss: 0.0879  | val_0_mae: 0.60838 |  0:00:00s\n",
      "epoch 9  | loss: 0.08276 | val_0_mae: 0.53846 |  0:00:00s\n",
      "epoch 10 | loss: 0.08057 | val_0_mae: 0.49264 |  0:00:00s\n",
      "epoch 11 | loss: 0.07249 | val_0_mae: 0.45871 |  0:00:00s\n",
      "epoch 12 | loss: 0.07449 | val_0_mae: 0.37725 |  0:00:00s\n",
      "epoch 13 | loss: 0.07075 | val_0_mae: 0.39097 |  0:00:00s\n",
      "epoch 14 | loss: 0.06576 | val_0_mae: 0.39674 |  0:00:00s\n",
      "epoch 15 | loss: 0.06821 | val_0_mae: 0.36741 |  0:00:00s\n",
      "epoch 16 | loss: 0.06582 | val_0_mae: 0.33916 |  0:00:00s\n",
      "epoch 17 | loss: 0.06456 | val_0_mae: 0.32681 |  0:00:00s\n",
      "epoch 18 | loss: 0.06638 | val_0_mae: 0.30052 |  0:00:00s\n",
      "epoch 19 | loss: 0.06479 | val_0_mae: 0.30613 |  0:00:00s\n",
      "epoch 20 | loss: 0.06459 | val_0_mae: 0.30315 |  0:00:00s\n",
      "epoch 21 | loss: 0.06526 | val_0_mae: 0.28173 |  0:00:00s\n",
      "epoch 22 | loss: 0.06358 | val_0_mae: 0.28177 |  0:00:01s\n",
      "epoch 23 | loss: 0.06263 | val_0_mae: 0.26602 |  0:00:01s\n",
      "epoch 24 | loss: 0.0629  | val_0_mae: 0.27813 |  0:00:01s\n",
      "epoch 25 | loss: 0.06354 | val_0_mae: 0.26701 |  0:00:01s\n",
      "epoch 26 | loss: 0.06274 | val_0_mae: 0.24674 |  0:00:01s\n",
      "epoch 27 | loss: 0.06207 | val_0_mae: 0.25882 |  0:00:01s\n",
      "epoch 28 | loss: 0.0625  | val_0_mae: 0.25749 |  0:00:01s\n",
      "epoch 29 | loss: 0.06141 | val_0_mae: 0.23418 |  0:00:01s\n",
      "epoch 30 | loss: 0.06145 | val_0_mae: 0.24181 |  0:00:01s\n",
      "epoch 31 | loss: 0.0612  | val_0_mae: 0.23178 |  0:00:01s\n",
      "epoch 32 | loss: 0.06042 | val_0_mae: 0.22779 |  0:00:01s\n",
      "epoch 33 | loss: 0.06097 | val_0_mae: 0.23307 |  0:00:01s\n",
      "epoch 34 | loss: 0.06106 | val_0_mae: 0.22941 |  0:00:01s\n",
      "epoch 35 | loss: 0.0607  | val_0_mae: 0.22215 |  0:00:01s\n",
      "epoch 36 | loss: 0.06099 | val_0_mae: 0.23047 |  0:00:01s\n",
      "epoch 37 | loss: 0.06277 | val_0_mae: 0.23028 |  0:00:01s\n",
      "epoch 38 | loss: 0.06222 | val_0_mae: 0.22465 |  0:00:01s\n",
      "epoch 39 | loss: 0.06217 | val_0_mae: 0.22364 |  0:00:01s\n",
      "epoch 40 | loss: 0.06179 | val_0_mae: 0.22602 |  0:00:01s\n",
      "epoch 41 | loss: 0.06097 | val_0_mae: 0.21437 |  0:00:01s\n",
      "epoch 42 | loss: 0.06146 | val_0_mae: 0.217   |  0:00:01s\n",
      "epoch 43 | loss: 0.06265 | val_0_mae: 0.21423 |  0:00:01s\n",
      "epoch 44 | loss: 0.05938 | val_0_mae: 0.20956 |  0:00:01s\n",
      "epoch 45 | loss: 0.06083 | val_0_mae: 0.21695 |  0:00:02s\n",
      "epoch 46 | loss: 0.06067 | val_0_mae: 0.20438 |  0:00:02s\n",
      "epoch 47 | loss: 0.06053 | val_0_mae: 0.20732 |  0:00:02s\n",
      "epoch 48 | loss: 0.06047 | val_0_mae: 0.20499 |  0:00:02s\n",
      "epoch 49 | loss: 0.05973 | val_0_mae: 0.20572 |  0:00:02s\n",
      "epoch 50 | loss: 0.06102 | val_0_mae: 0.20643 |  0:00:02s\n",
      "epoch 51 | loss: 0.05983 | val_0_mae: 0.20615 |  0:00:02s\n",
      "epoch 52 | loss: 0.0587  | val_0_mae: 0.20234 |  0:00:02s\n",
      "epoch 53 | loss: 0.05919 | val_0_mae: 0.20952 |  0:00:02s\n",
      "epoch 54 | loss: 0.06063 | val_0_mae: 0.20462 |  0:00:02s\n",
      "epoch 55 | loss: 0.06001 | val_0_mae: 0.20276 |  0:00:02s\n",
      "epoch 56 | loss: 0.06074 | val_0_mae: 0.20656 |  0:00:02s\n",
      "epoch 57 | loss: 0.06041 | val_0_mae: 0.20233 |  0:00:02s\n",
      "epoch 58 | loss: 0.06162 | val_0_mae: 0.20169 |  0:00:02s\n",
      "epoch 59 | loss: 0.06069 | val_0_mae: 0.2063  |  0:00:02s\n",
      "epoch 60 | loss: 0.0593  | val_0_mae: 0.20282 |  0:00:02s\n",
      "epoch 61 | loss: 0.05917 | val_0_mae: 0.20304 |  0:00:02s\n",
      "epoch 62 | loss: 0.06018 | val_0_mae: 0.20404 |  0:00:02s\n",
      "epoch 63 | loss: 0.06027 | val_0_mae: 0.20057 |  0:00:02s\n",
      "epoch 64 | loss: 0.0593  | val_0_mae: 0.20558 |  0:00:02s\n",
      "epoch 65 | loss: 0.05984 | val_0_mae: 0.20026 |  0:00:02s\n",
      "epoch 66 | loss: 0.05962 | val_0_mae: 0.20359 |  0:00:02s\n",
      "epoch 67 | loss: 0.05878 | val_0_mae: 0.20125 |  0:00:02s\n",
      "epoch 68 | loss: 0.06033 | val_0_mae: 0.20296 |  0:00:03s\n",
      "epoch 69 | loss: 0.05878 | val_0_mae: 0.20196 |  0:00:03s\n",
      "epoch 70 | loss: 0.05886 | val_0_mae: 0.20296 |  0:00:03s\n",
      "epoch 71 | loss: 0.05969 | val_0_mae: 0.2011  |  0:00:03s\n",
      "epoch 72 | loss: 0.05879 | val_0_mae: 0.20238 |  0:00:03s\n",
      "epoch 73 | loss: 0.05948 | val_0_mae: 0.2005  |  0:00:03s\n",
      "epoch 74 | loss: 0.05998 | val_0_mae: 0.20279 |  0:00:03s\n",
      "epoch 75 | loss: 0.05853 | val_0_mae: 0.20065 |  0:00:03s\n",
      "\n",
      "Early stopping occurred at epoch 75 with best_epoch = 65 and best_val_0_mae = 0.20026\n",
      "\n",
      "üîÅ Fold 8/10\n",
      "‚ñ∂ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Fine-tuning...\n",
      "epoch 0  | loss: 21.63481| val_0_mae: 1.0781  |  0:00:00s\n",
      "epoch 1  | loss: 3.09279 | val_0_mae: 0.77832 |  0:00:00s\n",
      "epoch 2  | loss: 0.96579 | val_0_mae: 0.94559 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.52167 | val_0_mae: 0.75642 |  0:00:00s\n",
      "epoch 4  | loss: 0.23327 | val_0_mae: 0.6464  |  0:00:00s\n",
      "epoch 5  | loss: 0.1796  | val_0_mae: 0.56044 |  0:00:00s\n",
      "epoch 6  | loss: 0.13187 | val_0_mae: 0.39991 |  0:00:00s\n",
      "epoch 7  | loss: 0.11129 | val_0_mae: 0.43814 |  0:00:00s\n",
      "epoch 8  | loss: 0.09356 | val_0_mae: 0.29762 |  0:00:00s\n",
      "epoch 9  | loss: 0.08548 | val_0_mae: 0.30705 |  0:00:00s\n",
      "epoch 10 | loss: 0.07912 | val_0_mae: 0.27592 |  0:00:00s\n",
      "epoch 11 | loss: 0.07355 | val_0_mae: 0.27796 |  0:00:00s\n",
      "epoch 12 | loss: 0.07419 | val_0_mae: 0.22442 |  0:00:00s\n",
      "epoch 13 | loss: 0.07264 | val_0_mae: 0.24702 |  0:00:00s\n",
      "epoch 14 | loss: 0.06857 | val_0_mae: 0.23935 |  0:00:00s\n",
      "epoch 15 | loss: 0.0684  | val_0_mae: 0.22726 |  0:00:00s\n",
      "epoch 16 | loss: 0.06633 | val_0_mae: 0.22324 |  0:00:00s\n",
      "epoch 17 | loss: 0.06619 | val_0_mae: 0.22367 |  0:00:00s\n",
      "epoch 18 | loss: 0.06401 | val_0_mae: 0.22105 |  0:00:00s\n",
      "epoch 19 | loss: 0.06501 | val_0_mae: 0.22392 |  0:00:00s\n",
      "epoch 20 | loss: 0.06286 | val_0_mae: 0.21656 |  0:00:01s\n",
      "epoch 21 | loss: 0.06362 | val_0_mae: 0.2223  |  0:00:01s\n",
      "epoch 22 | loss: 0.06365 | val_0_mae: 0.21972 |  0:00:01s\n",
      "epoch 23 | loss: 0.06369 | val_0_mae: 0.22095 |  0:00:01s\n",
      "epoch 24 | loss: 0.064   | val_0_mae: 0.21451 |  0:00:01s\n",
      "epoch 25 | loss: 0.06217 | val_0_mae: 0.21696 |  0:00:01s\n",
      "epoch 26 | loss: 0.06216 | val_0_mae: 0.21035 |  0:00:01s\n",
      "epoch 27 | loss: 0.06365 | val_0_mae: 0.20968 |  0:00:01s\n",
      "epoch 28 | loss: 0.06262 | val_0_mae: 0.2114  |  0:00:01s\n",
      "epoch 29 | loss: 0.06192 | val_0_mae: 0.2061  |  0:00:01s\n",
      "epoch 30 | loss: 0.06332 | val_0_mae: 0.2061  |  0:00:01s\n",
      "epoch 31 | loss: 0.0619  | val_0_mae: 0.20951 |  0:00:01s\n",
      "epoch 32 | loss: 0.06212 | val_0_mae: 0.20965 |  0:00:01s\n",
      "epoch 33 | loss: 0.0613  | val_0_mae: 0.20755 |  0:00:01s\n",
      "epoch 34 | loss: 0.06204 | val_0_mae: 0.2108  |  0:00:01s\n",
      "epoch 35 | loss: 0.06176 | val_0_mae: 0.20993 |  0:00:01s\n",
      "epoch 36 | loss: 0.06042 | val_0_mae: 0.20989 |  0:00:01s\n",
      "epoch 37 | loss: 0.06091 | val_0_mae: 0.21247 |  0:00:01s\n",
      "epoch 38 | loss: 0.06069 | val_0_mae: 0.21398 |  0:00:01s\n",
      "epoch 39 | loss: 0.05942 | val_0_mae: 0.21241 |  0:00:01s\n",
      "epoch 40 | loss: 0.06154 | val_0_mae: 0.21204 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 40 with best_epoch = 30 and best_val_0_mae = 0.2061\n",
      "\n",
      "üîÅ Fold 9/10\n",
      "‚ñ∂ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Fine-tuning...\n",
      "epoch 0  | loss: 15.7881 | val_0_mae: 1.90869 |  0:00:00s\n",
      "epoch 1  | loss: 2.05157 | val_0_mae: 0.89672 |  0:00:00s\n",
      "epoch 2  | loss: 0.55127 | val_0_mae: 0.66361 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.27287 | val_0_mae: 0.50429 |  0:00:00s\n",
      "epoch 4  | loss: 0.14112 | val_0_mae: 0.44656 |  0:00:00s\n",
      "epoch 5  | loss: 0.10665 | val_0_mae: 0.34357 |  0:00:00s\n",
      "epoch 6  | loss: 0.08628 | val_0_mae: 0.23959 |  0:00:00s\n",
      "epoch 7  | loss: 0.07728 | val_0_mae: 0.25752 |  0:00:00s\n",
      "epoch 8  | loss: 0.07184 | val_0_mae: 0.24558 |  0:00:00s\n",
      "epoch 9  | loss: 0.07168 | val_0_mae: 0.22289 |  0:00:00s\n",
      "epoch 10 | loss: 0.06634 | val_0_mae: 0.2394  |  0:00:00s\n",
      "epoch 11 | loss: 0.06364 | val_0_mae: 0.24462 |  0:00:00s\n",
      "epoch 12 | loss: 0.06418 | val_0_mae: 0.22706 |  0:00:00s\n",
      "epoch 13 | loss: 0.06242 | val_0_mae: 0.21251 |  0:00:00s\n",
      "epoch 14 | loss: 0.06142 | val_0_mae: 0.21476 |  0:00:00s\n",
      "epoch 15 | loss: 0.06157 | val_0_mae: 0.21294 |  0:00:00s\n",
      "epoch 16 | loss: 0.06135 | val_0_mae: 0.21452 |  0:00:00s\n",
      "epoch 17 | loss: 0.06097 | val_0_mae: 0.2132  |  0:00:00s\n",
      "epoch 18 | loss: 0.06089 | val_0_mae: 0.20859 |  0:00:00s\n",
      "epoch 19 | loss: 0.06202 | val_0_mae: 0.21147 |  0:00:00s\n",
      "epoch 20 | loss: 0.0598  | val_0_mae: 0.20648 |  0:00:00s\n",
      "epoch 21 | loss: 0.06096 | val_0_mae: 0.20648 |  0:00:01s\n",
      "epoch 22 | loss: 0.06071 | val_0_mae: 0.2096  |  0:00:01s\n",
      "epoch 23 | loss: 0.05963 | val_0_mae: 0.20049 |  0:00:01s\n",
      "epoch 24 | loss: 0.05995 | val_0_mae: 0.2066  |  0:00:01s\n",
      "epoch 25 | loss: 0.06121 | val_0_mae: 0.20376 |  0:00:01s\n",
      "epoch 26 | loss: 0.06001 | val_0_mae: 0.20081 |  0:00:01s\n",
      "epoch 27 | loss: 0.06022 | val_0_mae: 0.20696 |  0:00:01s\n",
      "epoch 28 | loss: 0.06004 | val_0_mae: 0.203   |  0:00:01s\n",
      "epoch 29 | loss: 0.06098 | val_0_mae: 0.20242 |  0:00:01s\n",
      "epoch 30 | loss: 0.05933 | val_0_mae: 0.20264 |  0:00:01s\n",
      "epoch 31 | loss: 0.06025 | val_0_mae: 0.19979 |  0:00:01s\n",
      "epoch 32 | loss: 0.05934 | val_0_mae: 0.20147 |  0:00:01s\n",
      "epoch 33 | loss: 0.05968 | val_0_mae: 0.20101 |  0:00:01s\n",
      "epoch 34 | loss: 0.05938 | val_0_mae: 0.20197 |  0:00:01s\n",
      "epoch 35 | loss: 0.05913 | val_0_mae: 0.20401 |  0:00:01s\n",
      "epoch 36 | loss: 0.05945 | val_0_mae: 0.20222 |  0:00:01s\n",
      "epoch 37 | loss: 0.0598  | val_0_mae: 0.20464 |  0:00:01s\n",
      "epoch 38 | loss: 0.05856 | val_0_mae: 0.20199 |  0:00:01s\n",
      "epoch 39 | loss: 0.05915 | val_0_mae: 0.20466 |  0:00:01s\n",
      "epoch 40 | loss: 0.05858 | val_0_mae: 0.20394 |  0:00:01s\n",
      "epoch 41 | loss: 0.05902 | val_0_mae: 0.20331 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 41 with best_epoch = 31 and best_val_0_mae = 0.19979\n",
      "\n",
      "üîÅ Fold 10/10\n",
      "‚ñ∂ Pretraining...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ñ∂ Fine-tuning...\n",
      "epoch 0  | loss: 3.45799 | val_0_mae: 0.52669 |  0:00:00s\n",
      "epoch 1  | loss: 0.41193 | val_0_mae: 0.43101 |  0:00:00s\n",
      "epoch 2  | loss: 0.20684 | val_0_mae: 0.47479 |  0:00:00s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
      "  warnings.warn(f\"Device used : {self.device}\")\n",
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:248: UserWarning: Loading weights from unsupervised pretraining\n",
      "  warnings.warn(\"Loading weights from unsupervised pretraining\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 3  | loss: 0.12299 | val_0_mae: 0.28763 |  0:00:00s\n",
      "epoch 4  | loss: 0.10577 | val_0_mae: 0.25542 |  0:00:00s\n",
      "epoch 5  | loss: 0.08835 | val_0_mae: 0.24428 |  0:00:00s\n",
      "epoch 6  | loss: 0.07895 | val_0_mae: 0.23209 |  0:00:00s\n",
      "epoch 7  | loss: 0.07471 | val_0_mae: 0.22482 |  0:00:00s\n",
      "epoch 8  | loss: 0.06727 | val_0_mae: 0.21613 |  0:00:00s\n",
      "epoch 9  | loss: 0.06767 | val_0_mae: 0.22197 |  0:00:00s\n",
      "epoch 10 | loss: 0.06579 | val_0_mae: 0.22368 |  0:00:00s\n",
      "epoch 11 | loss: 0.06378 | val_0_mae: 0.21551 |  0:00:00s\n",
      "epoch 12 | loss: 0.06067 | val_0_mae: 0.21389 |  0:00:00s\n",
      "epoch 13 | loss: 0.06246 | val_0_mae: 0.21541 |  0:00:00s\n",
      "epoch 14 | loss: 0.06144 | val_0_mae: 0.2126  |  0:00:00s\n",
      "epoch 15 | loss: 0.06078 | val_0_mae: 0.21452 |  0:00:00s\n",
      "epoch 16 | loss: 0.06194 | val_0_mae: 0.21255 |  0:00:00s\n",
      "epoch 17 | loss: 0.05932 | val_0_mae: 0.21313 |  0:00:00s\n",
      "epoch 18 | loss: 0.0602  | val_0_mae: 0.21003 |  0:00:00s\n",
      "epoch 19 | loss: 0.06151 | val_0_mae: 0.21084 |  0:00:00s\n",
      "epoch 20 | loss: 0.05956 | val_0_mae: 0.21354 |  0:00:00s\n",
      "epoch 21 | loss: 0.05957 | val_0_mae: 0.21208 |  0:00:01s\n",
      "epoch 22 | loss: 0.06013 | val_0_mae: 0.20933 |  0:00:01s\n",
      "epoch 23 | loss: 0.06001 | val_0_mae: 0.21072 |  0:00:01s\n",
      "epoch 24 | loss: 0.0602  | val_0_mae: 0.21243 |  0:00:01s\n",
      "epoch 25 | loss: 0.05923 | val_0_mae: 0.21115 |  0:00:01s\n",
      "epoch 26 | loss: 0.0594  | val_0_mae: 0.21165 |  0:00:01s\n",
      "epoch 27 | loss: 0.0586  | val_0_mae: 0.2096  |  0:00:01s\n",
      "epoch 28 | loss: 0.05831 | val_0_mae: 0.21137 |  0:00:01s\n",
      "epoch 29 | loss: 0.05827 | val_0_mae: 0.21122 |  0:00:01s\n",
      "epoch 30 | loss: 0.05883 | val_0_mae: 0.20926 |  0:00:01s\n",
      "epoch 31 | loss: 0.05965 | val_0_mae: 0.21252 |  0:00:01s\n",
      "epoch 32 | loss: 0.05898 | val_0_mae: 0.21271 |  0:00:01s\n",
      "epoch 33 | loss: 0.05737 | val_0_mae: 0.21332 |  0:00:01s\n",
      "epoch 34 | loss: 0.05821 | val_0_mae: 0.21074 |  0:00:01s\n",
      "epoch 35 | loss: 0.0583  | val_0_mae: 0.21278 |  0:00:01s\n",
      "epoch 36 | loss: 0.05904 | val_0_mae: 0.21217 |  0:00:01s\n",
      "epoch 37 | loss: 0.05865 | val_0_mae: 0.21326 |  0:00:01s\n",
      "epoch 38 | loss: 0.05922 | val_0_mae: 0.21352 |  0:00:01s\n",
      "epoch 39 | loss: 0.05792 | val_0_mae: 0.21429 |  0:00:01s\n",
      "epoch 40 | loss: 0.05839 | val_0_mae: 0.21265 |  0:00:01s\n",
      "\n",
      "Early stopping occurred at epoch 40 with best_epoch = 30 and best_val_0_mae = 0.20926\n",
      "\n",
      "‚úÖ Î™®Îì† fold Î™®Îç∏ ÌïôÏäµ ÏôÑÎ£å!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hyun/torch_venv/lib/python3.12/site-packages/pytorch_tabnet/callbacks.py:172: UserWarning: Best weights from best epoch are automatically used!\n",
      "  warnings.warn(wrn_msg)\n"
     ]
    }
   ],
   "source": [
    "# ÌÉÄÍ≤ü ÏßÄÏ†ï\n",
    "target = train['ÏÑ±Í≥µÌôïÎ•†']  \n",
    "X = train[features]\n",
    "y = target\n",
    "\n",
    "# KFold ÏÑ§Ï†ï\n",
    "N_FOLDS = 10\n",
    "kf = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\n",
    "\n",
    "models = [] # Î™®Îç∏ Ï†ÄÏû• Î¶¨Ïä§Ìä∏\n",
    "cv_scores = []\n",
    "\n",
    "for fold, (train_idx, valid_idx) in enumerate(kf.split(X)):\n",
    "    print(f\"\\nüîÅ Fold {fold+1}/{N_FOLDS}\")\n",
    "    \n",
    "    X_train = X.iloc[train_idx].values\n",
    "    y_train = y.iloc[train_idx].values.reshape(-1, 1)\n",
    "    \n",
    "    X_valid = X.iloc[valid_idx].values\n",
    "    y_valid = y.iloc[valid_idx].values.reshape(-1, 1)\n",
    "    \n",
    "    # ÎπÑÏßÄÎèÑ ÏÇ¨Ï†ÑÌïôÏäµ\n",
    "    print(\"‚ñ∂ Pretraining...\")\n",
    "\n",
    "    pretrainer = TabNetPretrainer(\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        seed=42,\n",
    "        verbose=0\n",
    "    )\n",
    "\n",
    "    pretrainer.fit(\n",
    "        X_train=X_train,\n",
    "        max_epochs=100,\n",
    "        batch_size=512,\n",
    "        virtual_batch_size=64\n",
    "    )\n",
    "\n",
    "\n",
    "    # ÏßÄÎèÑ ÌïôÏäµ \n",
    "    print(\"‚ñ∂ Fine-tuning...\")\n",
    "    model = TabNetRegressor(\n",
    "        cat_idxs=cat_idxs,\n",
    "        cat_dims=cat_dims,\n",
    "        seed=42,\n",
    "        verbose=1,       \n",
    "        optimizer_fn=torch.optim.AdamW,\n",
    "    )\n",
    "\n",
    "    model.fit(\n",
    "        X_train=X_train, y_train=y_train,\n",
    "        eval_set=[(X_valid, y_valid)],\n",
    "        from_unsupervised=pretrainer,\n",
    "        eval_metric=['mae'],\n",
    "        max_epochs=100,\n",
    "        patience=10,\n",
    "    )\n",
    "\n",
    "    # Î™®Îç∏ÏùÑ Î©îÎ™®Î¶¨Ïóê Ï†ÄÏû•\n",
    "    models.append(model)\n",
    "    cv_scores.append(model.best_cost)\n",
    "\n",
    "print(\"\\n‚úÖ Î™®Îì† fold Î™®Îç∏ ÌïôÏäµ ÏôÑÎ£å!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predict with fold 1\n",
      "Predict with fold 2\n",
      "Predict with fold 3\n",
      "Predict with fold 4\n",
      "Predict with fold 5\n",
      "Predict with fold 6\n",
      "Predict with fold 7\n",
      "Predict with fold 8\n",
      "Predict with fold 9\n",
      "Predict with fold 10\n"
     ]
    }
   ],
   "source": [
    "# Ï†ÄÏû•Îêú Î™®Îç∏Îì§Î°ú ÏòàÏ∏°\n",
    "predictions_list = []\n",
    "\n",
    "for fold, model in enumerate(models):\n",
    "    print(f\"Predict with fold {fold+1}\")\n",
    "    preds = model.predict(test[features].values)\n",
    "    predictions_list.append(preds)\n",
    "\n",
    "# ÌèâÍ∑† ÏòàÏ∏°\n",
    "final_predictions = np.mean(predictions_list, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_submission['ÏÑ±Í≥µÌôïÎ•†'] = final_predictions\n",
    "sample_submission.to_csv('./baseline_submission5.csv', index = False, encoding = 'utf-8-sig')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           Feature  Importance\n",
      "5             ÏÉÅÏû•Ïó¨Î∂Ä    0.516250\n",
      "4             Ïù∏ÏàòÏó¨Î∂Ä    0.244577\n",
      "11              Ïó∞Ï∞®    0.136009\n",
      "3             ÏßÅÏõê Ïàò    0.022742\n",
      "1               Î∂ÑÏïº    0.021302\n",
      "10       Í∏∞ÏóÖÍ∞ÄÏπò(Î∞±ÏñµÏõê)    0.020834\n",
      "9   SNS ÌåîÎ°úÏõå Ïàò(Î∞±ÎßåÎ™Ö)    0.016876\n",
      "8          Ïó∞Îß§Ï∂ú(ÏñµÏõê)    0.014887\n",
      "2             Ìà¨ÏûêÎã®Í≥Ñ    0.003031\n",
      "6         Í≥†Í∞ùÏàò(Î∞±ÎßåÎ™Ö)    0.001531\n",
      "7        Ï¥ù Ìà¨ÏûêÍ∏à(ÏñµÏõê)    0.000988\n",
      "0               Íµ≠Í∞Ä    0.000975\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Î™®Îç∏ÏóêÏÑú Ï§ëÏöîÎèÑ Ï∂îÏ∂ú (Ïòà: Ï≤´ Î≤àÏß∏ fold Î™®Îç∏ ÏÇ¨Ïö©)\n",
    "importances = models[0].feature_importances_\n",
    "\n",
    "# Ïª¨ÎüºÎ™ÖÍ≥º Îß§Ìïë\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    'Feature': features,\n",
    "    'Importance': importances\n",
    "})\n",
    "\n",
    "# Ï§ëÏöîÎèÑ Í∏∞Ï§Ä Ï†ïÎ†¨\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "# Í≤∞Í≥º Ï∂úÎ†•\n",
    "print(feature_importance_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
